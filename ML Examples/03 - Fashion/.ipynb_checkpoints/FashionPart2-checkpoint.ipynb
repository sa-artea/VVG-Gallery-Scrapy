{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REDES NEURONALES PROFUNDAS\n",
    "Utilice el conjunto de datos Fashion-MNIST para construir un clasificador de imágenes de productos. Para la construcción del modelo utilice los tres esquemas que se describen a continuación y compare los resultados:\n",
    "\n",
    "1. Entrenamiento de un perceptrón multicapa.\n",
    "2. Entrenamiento de un undercompleted autoencoder para realizar una reducción de la dimensionalidad. Sobre el nuevo conjunto de características construya el modelo de clasificación con un perceptrón multicapa.\n",
    "3. Entrenamiento de un denoising autoendoder para preentrenar una red profunda. Reuse lasprimeras capas de este autoencoder para construir un perceptron multicapa (con solo el 10% de los datos).\n",
    "\n",
    "Para los puntos 2) y 3) compruebe, y muestre con ejemplos, que las imágenes están bien reconstruidas.\n",
    "\n",
    "## OBJETIVO\n",
    "Aplicar el proceso de aprendizaje a partir de datos para resolver problemas de clasificación utilizando redes neuronales profundas, sobre la herramienta Keras.\n",
    "\n",
    "## DATOS\n",
    "Incluidos en Keras.\n",
    "Tambien, existe otra fuente equivalente que se consigue en el siguiente URL https://www.kaggle.com/zalando-research/fashionmnist donde hay un resumen de estos datos en el archivo CVS y XLSX.\n",
    "\n",
    "la clasificacion para el aprendizaje supervisado es:\n",
    "\n",
    "    Label \tClass\n",
    "    0 \t \tT-shirt/top\n",
    "    1 \t \tTrouser\n",
    "    2 \t \tPullover\n",
    "    3 \t \tDress\n",
    "    4 \t \tCoat\n",
    "    5 \t \tSandal\n",
    "    6 \t \tShirt\n",
    "    7 \t \tSneaker\n",
    "    8 \t \tBag\n",
    "    9 \t \tAnkle boot\n",
    "\n",
    "**Importante: justifique cada decisión tomada y presente un análisis de los resultados obtenidos.**\n",
    "### Consideraciones\n",
    "- Utilice sólo los conjuntos de datos que se indican.\n",
    "- El frameworks a utilizar es Keras con Jupyter Notebbooks.\n",
    "- El taller debe ser realizado en grupos de tres personas.\n",
    "- Entregables: archivos ipynb (comentados, con la justificación de las decisiones consideradas y análisis de resultados).\n",
    "\n",
    "### Enlaces de Interes\n",
    "- Ejemplos de Talos https://nbviewer.jupyter.org/github/autonomio/talos/tree/master/examples/\n",
    "- Articulo sobre el uso de Talos https://towardsdatascience.com/hyperparameter-optimization-with-keras-b82e6364ca53\n",
    "- Portal oficial de Talos https://autonomio.github.io/docs_talos/#introduction\n",
    "- Github Talos https://github.com/autonomio/talos/tree/master/talos/scan\n",
    "- Documentacion de github Talos https://github.com/autonomio/talos/tree/master/docs\n",
    "- Documentacion Keras https://keras.io/models/sequential/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importando dependencias de trabajo\n",
    "# se importa OS, pandas y numpy para leer y preparar datos\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from pandas import ExcelWriter\n",
    "from pandas import ExcelFile\n",
    "import pandas_profiling as profile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "\n",
    "# los modelos de aprendizaje con las diferentes librerias\n",
    "import talos\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import InputLayer\n",
    "from keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# otras librerias para manejo de experimentos, pruebas, optimizacion y otros\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from keras.utils import to_categorical\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar archivos\n",
    "Para cargar los datos necesarios se siguen los siguientes pasos:\n",
    "\n",
    " - Se cargan los datos desde el conjunto de datos incluido en Keras\n",
    "\n",
    " - Se crean diccionarios de referencia para el pruebas y entrenamiento.\n",
    " - Se leen los arhivos de pruebas y entrenamiento para crear un set de datos.\n",
    " - Se transforma cada uno de los archivos en matrices de datos y se adicionan a los conjuntos de pruebas y entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de nombres de las clasificaciones\n",
    "classNamesList = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se carga el archivo de datos de trabajo por medio de Keras\n",
    "fashionSource = keras.datasets.fashion_mnist\n",
    "(X_train, y_train), (X_test, y_test) = fashionSource.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nombres de columnas para el dataframe de pandas\n",
    "columns = [\"ID\", \"Data\", \"NormData\", \"ReshapeData\", \"Label\", \"Class\", \"DataSize\", \"ReshapeSize\"]\n",
    "X_Data = np.concatenate((X_train, X_test), axis = 0)\n",
    "y_Data = np.concatenate((y_train, y_test), axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# puedo hacer otra forma de aprendizaje reorganizando los grupos de prueba y entrenamiento para ver si existe algun cambio de comportamiento.\n",
    "# dataset para entrenamiento\n",
    "trainData = pd.DataFrame()\n",
    "\n",
    "# preparar datos dataset entrenamiento\n",
    "for col in columns:\n",
    "    \n",
    "    # creo el ID del set\n",
    "    if col is \"ID\":\n",
    "        \n",
    "        IDList = list(range(len(X_Data)))\n",
    "        trainData[col] = IDList\n",
    "\n",
    "    # datos de la imagen (nxm) para utilizarlos en el algoritmo de aprendizaje\n",
    "    elif col is \"Data\":\n",
    "        \n",
    "        dataList = np.array(X_Data).astype(float)\n",
    "        trainData[col] = list(dataList)\n",
    "\n",
    "    # datos de la imagen (nxm) para utilizarlos en el algoritmo de aprendizaje\n",
    "    elif col is \"NormData\":\n",
    "        \n",
    "        normList = np.array(X_Data).astype(float)\n",
    "        maxValue = normList.max()\n",
    "        normList = normList/maxValue\n",
    "        trainData[col] = list(normList)\n",
    "\n",
    "    # datos de la imagen (nxm) de con cambio de forma(reshape) para poder ser utilizados en el algoritmo de aprendizaje\n",
    "    elif col is \"ReshapeData\":\n",
    "        \n",
    "        reshapeData = list()\n",
    "        \n",
    "        for data in list(trainData[\"NormData\"]):\n",
    "            \n",
    "            tempImg = data.astype(float)\n",
    "            reshapeData.append(list(tempImg))\n",
    "        \n",
    "        reshapeData = np.array(reshapeData)\n",
    "        reshapeData = reshapeData.reshape(len(reshapeData), -1)\n",
    "        trainData[col] = list(reshapeData)\n",
    "        \n",
    "    # categoria de la imagen en entero\n",
    "    elif col is \"Label\":\n",
    "        \n",
    "        nameList = list(y_Data)\n",
    "        trainData[col] = nameList\n",
    "    \n",
    "    # nombre de la categoria/clase de la imagen\n",
    "    elif col is \"Class\":\n",
    "        \n",
    "        classList = list()\n",
    "        \n",
    "        for data in np.nditer(y_Data):\n",
    "            \n",
    "            tempClass = classNamesList[data]\n",
    "            classList.append(tempClass)\n",
    "        \n",
    "        classList = np.array(classList)\n",
    "        trainData[col] = list(classList)\n",
    "\n",
    "    # tamanho de pixeles de la matriz que representa la imagen (nxm)\n",
    "    elif col is \"DataSize\":\n",
    "\n",
    "        sizeList = list()\n",
    "                \n",
    "        for data in list(X_Data):\n",
    "            \n",
    "            tempSize = data.shape\n",
    "            sizeList.append(tempSize)\n",
    "        \n",
    "        sizeList = np.array(sizeList)\n",
    "        trainData[col] = list(sizeList)\n",
    "    \n",
    "    # tamanho dee pixales en 1D\n",
    "    elif col is \"ReshapeSize\":\n",
    "        \n",
    "        reshapeList = list()\n",
    "        dataList = trainData[\"ReshapeData\"]\n",
    "        dataList = np.array(dataList)\n",
    "        dataList = list(dataList)\n",
    "\n",
    "        for data in dataList:\n",
    "            \n",
    "            tempReshape = np.array(data.shape)\n",
    "            reshapeList.append(tempReshape)\n",
    "\n",
    "        dataList = np.array(reshapeList)\n",
    "        trainData[col] = list(reshapeList)#.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Data</th>\n",
       "      <th>NormData</th>\n",
       "      <th>ReshapeData</th>\n",
       "      <th>Label</th>\n",
       "      <th>Class</th>\n",
       "      <th>DataSize</th>\n",
       "      <th>ReshapeSize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>Ankle boot</td>\n",
       "      <td>[28, 28]</td>\n",
       "      <td>[784]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098,...</td>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "      <td>[28, 28]</td>\n",
       "      <td>[784]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "      <td>[28, 28]</td>\n",
       "      <td>[784]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.129...</td>\n",
       "      <td>3</td>\n",
       "      <td>Dress</td>\n",
       "      <td>[28, 28]</td>\n",
       "      <td>[784]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "      <td>[28, 28]</td>\n",
       "      <td>[784]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                               Data  \\\n",
       "0   0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1   1  [[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...   \n",
       "2   2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3   3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 33.0...   \n",
       "4   4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                            NormData  \\\n",
       "0  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "1  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098...   \n",
       "2  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "3  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.12...   \n",
       "4  [[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,...   \n",
       "\n",
       "                                         ReshapeData  Label        Class  \\\n",
       "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      9   Ankle boot   \n",
       "1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098,...      0  T-shirt/top   \n",
       "2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0  T-shirt/top   \n",
       "3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.129...      3        Dress   \n",
       "4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0  T-shirt/top   \n",
       "\n",
       "   DataSize ReshapeSize  \n",
       "0  [28, 28]       [784]  \n",
       "1  [28, 28]       [784]  \n",
       "2  [28, 28]       [784]  \n",
       "3  [28, 28]       [784]  \n",
       "4  [28, 28]       [784]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probando def de dataframe entrenamiento\n",
    "trainData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID              int64\n",
       "Data           object\n",
       "NormData       object\n",
       "ReshapeData    object\n",
       "Label           int64\n",
       "Class          object\n",
       "DataSize       object\n",
       "ReshapeSize    object\n",
       "dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# probando tipos de dataframe entrenamiento\n",
    "trainData.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AUTOCODIFICADOR (AUTOENCODER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se remueve la clase a predecir y demas columnas innecesarias\n",
    "dropColumns = [\"DataSize\", \"ReshapeSize\", \"Data\", \"NormData\"]\n",
    "encoderData = trainData.drop(columns = dropColumns, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ReshapeData</th>\n",
       "      <th>Label</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>9</td>\n",
       "      <td>Ankle boot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098,...</td>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.129...</td>\n",
       "      <td>3</td>\n",
       "      <td>Dress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>T-shirt/top</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID                                        ReshapeData  Label        Class\n",
       "0   0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      9   Ankle boot\n",
       "1   1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.00392156862745098,...      0  T-shirt/top\n",
       "2   2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0  T-shirt/top\n",
       "3   3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.129...      3        Dress\n",
       "4   4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...      0  T-shirt/top"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pruebo nueva representacion de los datos\n",
    "encoderData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# asigno variables para el entrenamiento del autoencoder\n",
    "X_encoder = encoderData[\"ReshapeData\"].tolist()\n",
    "y_encoder = encoderData[\"ReshapeData\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrego las capas necesarias para el modelo Keras para el Autoencoder\n",
    "# parametros para las capas\n",
    "neurons = trainData[\"ReshapeSize\"][0][0]\n",
    "output = trainData[\"ReshapeSize\"][0][0]\n",
    "inn = \"relu\"\n",
    "act = \"relu\"\n",
    "out = \"relu\" # \"relu\"\n",
    "ldrop = 0.2\n",
    "\n",
    "# forma de entrada de los datos para las capas de entrada, salida y aprendizaje\n",
    "# inshape = list(trainData[\"DataSize\"][0],)\n",
    "inshape = list(trainData[\"ReshapeSize\"][0],)\n",
    "outshape = list(trainData[\"ReshapeSize\"][0],)\n",
    "lshape = list(trainData[\"ReshapeSize\"][0],)\n",
    "\n",
    "# parametros para compilar\n",
    "# se selecciona Mean Squared Error (MSE) con datos estandarizados (imagen en escala de grises 0 a 255)\n",
    "l = \"mse\"\n",
    "opti = \"adam\" # \"adam\" \"adadelta\" \"nadam\"\n",
    "met = [\"accuracy\"]\n",
    "\n",
    "# parametros para mirar el progreso del entrenamiento\n",
    "ver = 1\n",
    "epo = 10\n",
    "batch = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de las capas del AUTOENCODER\n",
    "Para el AUTOENCODER se definen las siguientes capas:\n",
    "\n",
    "-\tUna capa de entrada “Dense” con 784 neuronas para la representación alternativa de los datos en 1D y función de activación “relu”.\n",
    "-\tUna primera capa de codificación  “Dense” con 392 neuronas a la representación 1D de los datos.\n",
    "-\tUna capa intermedia de hallazgos/aprendizaje “Dense” con 196 neuronas a la representación 1D de los datos.\n",
    "-\tUna primera capa de decodificación  “Dense” con 392 neuronas a la representación 1D de los datos.\n",
    "-\tUna capa de salida “Dense” con 784 neuronas para la representación alternativa de los datos en 1D y función de activación “relu”.\n",
    "-\tUnas capas “Dropout” entre cada capa de aprendizaje (codificación y decodificación) con 20% de desactivación de los datos de entrada para evitar un sobre ajuste del modelo.\n",
    "-\tTodas las capas de aprendizaje (codificación, decodificación e intermania) tiene una funcion de activación “relu” para evitar la saturación de la salida de las capas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de capas Keras para el AUTOENCODER\n",
    "layerList = (\n",
    "    # capa de entrada\n",
    "    Dense(neurons, input_shape = inshape, activation = inn),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "\n",
    "    # primera capa de codificacion neurons = 256\n",
    "    Dense(2**int(neurons**(0.5)-20), input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "\n",
    "    # segunda capa de codificacion neurons = 128\n",
    "    Dense(2**int(neurons**(0.5)-21), input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "    \n",
    "    # capa intermedia neurons = 64\n",
    "    Dense(2**int(neurons**(0.5)-22), input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "\n",
    "    # primera capa de codificacion neurons = 128\n",
    "    Dense(2**int(neurons**(0.5)-21), input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "    \n",
    "    # segunda capa de decodificacion neurons = 256\n",
    "    Dense(2**int(neurons**(0.5)-20), input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "\n",
    "    # capa de salida\n",
    "    Dense(output, input_shape = outshape, activation = out),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division de poblacion de entrenamiento y pruebas con los datos en un orden alternativos\n",
    "X_trainE, X_testE, y_trainE, y_testE = train_test_split(X_encoder, y_encoder, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo el modelo MLP de Keras\n",
    "basicEncoder = Sequential(layerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilando las condiciones de optimizacion y ajuste del MLP Keras\n",
    "basicEncoder.compile(loss = l, optimizer = opti, metrics = met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 56000 samples, validate on 14000 samples\n",
      "Epoch 1/10\n",
      "56000/56000 [==============================] - 18s 324us/step - loss: 0.0488 - accuracy: 0.0058 - val_loss: 0.0360 - val_accuracy: 0.0156\n",
      "Epoch 2/10\n",
      "56000/56000 [==============================] - 18s 323us/step - loss: 0.0335 - accuracy: 0.0134 - val_loss: 0.0296 - val_accuracy: 0.0256\n",
      "Epoch 3/10\n",
      "56000/56000 [==============================] - 19s 339us/step - loss: 0.0304 - accuracy: 0.0142 - val_loss: 0.0267 - val_accuracy: 0.0184\n",
      "Epoch 4/10\n",
      "56000/56000 [==============================] - 19s 344us/step - loss: 0.0289 - accuracy: 0.0132 - val_loss: 0.0263 - val_accuracy: 0.0244\n",
      "Epoch 5/10\n",
      "56000/56000 [==============================] - 20s 350us/step - loss: 0.0277 - accuracy: 0.0127 - val_loss: 0.0247 - val_accuracy: 0.0186\n",
      "Epoch 6/10\n",
      "56000/56000 [==============================] - 19s 330us/step - loss: 0.0269 - accuracy: 0.0151 - val_loss: 0.0244 - val_accuracy: 0.0181\n",
      "Epoch 7/10\n",
      "56000/56000 [==============================] - 19s 348us/step - loss: 0.0263 - accuracy: 0.0156 - val_loss: 0.0231 - val_accuracy: 0.0208\n",
      "Epoch 8/10\n",
      "56000/56000 [==============================] - 18s 330us/step - loss: 0.0257 - accuracy: 0.0153 - val_loss: 0.0216 - val_accuracy: 0.0199\n",
      "Epoch 9/10\n",
      "56000/56000 [==============================] - 20s 350us/step - loss: 0.0253 - accuracy: 0.0168 - val_loss: 0.0215 - val_accuracy: 0.0171\n",
      "Epoch 10/10\n",
      "56000/56000 [==============================] - 19s 334us/step - loss: 0.0249 - accuracy: 0.0172 - val_loss: 0.0215 - val_accuracy: 0.0202\n"
     ]
    }
   ],
   "source": [
    "# ajustando el modelo MLP Keras\n",
    "historyEncoder = basicEncoder.fit(\n",
    "    x = np.array(X_trainE), \n",
    "    y = np.array(y_trainE), \n",
    "    epochs = epo, verbose = ver,\n",
    "    workers = 32, use_multiprocessing = True,\n",
    "    batch_size = batch,\n",
    "    validation_data = (np.array(X_testE), np.array(X_testE))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14000/14000 [==============================] - 2s 165us/step\n"
     ]
    }
   ],
   "source": [
    "# evaluando el modelo entrenado con sus propias herramientas\n",
    "evalEnconder = basicEncoder.evaluate(np.array(X_testE), np.array(X_testE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perdida:  0.021453702332718032\n"
     ]
    }
   ],
   "source": [
    "print(\"Perdida: \", evalEnconder[0])\n",
    "# print(\"Precision: \", evalEnconder[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 784)               615440    \n",
      "_________________________________________________________________\n",
      "dropout_12 (Dropout)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 256)               200960    \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 784)               201488    \n",
      "=================================================================\n",
      "Total params: 1,100,384\n",
      "Trainable params: 1,100,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# resumen de datos del MLP Keras\n",
    "basicEncoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copio el modelo entrenado en otro modelo para removerle las capas despues de la intermedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basicEncoder.save('my_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model2 = load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se prueba Autoencoder con keras\n",
    "encoderPrediction = basicEncoder.predict(np.array(X_testE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxImages = 10\n",
    "randomTestImg = np.random.randint(len(X_testE), size = maxImages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA/4AAADrCAYAAADQb014AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO29edS1ZVm//6E0M0VJRAFRJgEFBJTBAecRB8x5WeZQWdlSyaxsMHV9W6JN5mpa6cqKLDPBWRNFBEUcEEFQlFl4AZlFRa1Mjd8f/I77Pvf1XM/meeF9X5597+P459nPPe59jfd9nZ/zPLe64YYbIiIiIiIiIiLT5Cdu7S8gIiIiIiIiIpsPX/xFREREREREJowv/iIiIiIiIiITxhd/ERERERERkQnji7+IiIiIiIjIhPHFX0RERERERGTC3GZzXHSrrbZa6hyBN9xww1a39nfoYb2sz3pJNm/dbLXVjT+7l7rzlFNOSZJsvfXWSZL73//+w74f/OAHN3n+pmK91o19Zlr1spa2fOihhyZJDjvssCTJa17zmjVd+w1veEOS5MMf/nCS5LOf/ewm/24wtXq5iWsmWVku9773vYfPD3zgA5Mkd7zjHVecf9555yVJTjzxxJntP/mTPzl8/r//+7/uPTaWqdbLT/zEjTYiymke73//+4fPu+66a5Lkf/7nf5Ik22677bDv4osvTpI89rGPnTn/tre97fD5hz/84c37wg1TrZdFZ5nqZbVx7J//+Z+Hz7vsskuS5Da3ufHV7Lrrrhv2/dzP/dyarrcpWK/1kthnNkXdaPEXERERERERmTCbxeK/JWBV+RGPeMSw7eqrr06S3P72t08yu1rMSvP555+fJDn44IO3yPecAj/1Uz+VJNlzzz2TJE972tOGfa9//etnjsUy0GMt1gK4xz3uMXy+3/3ulyT53Oc+lyT5zne+c5Pn1++xMfedIpTFj3/84ySzfWannXZKMpbRox/96GHfsccem2TLWPxFNhe032RlG37MYx4zfH7lK1+ZZBx7vvnNbyaZtbpce+21Sca55V73utew79RTT02SPOEJT0iS/Nd//VeS5I/+6I+GYz71qU+t+bvJjbTl8vu///tJZueIt771rUmSs846K8moYEqSl770pUmS5z3veUmSF7/4xUnG8TAZ57j//d//3aTffRGpcydlP28Ovd3tbpck+b3f+70ks5bJSy65ZNVr0/d+6Zd+Kclo+dxUVn6RLc28Z6V22+67754kudOd7jRs+/KXv5xkHI+23377Yd/Tn/70JMn73ve+Ve+xlu8hosVfREREREREZML44i8iIiIiIiIyYbbaHFKQzRl8gYA8BIZB1p+slKX1fhsBy5A0X3jhhZv8O67XwBjz6gUZ/4EHHpgkuctd7jLsQ+p66aWXJkle/vKXD/suv/zyJMlv/dZvbZLvePe73z1J8s53vnPY9h//8R8zxyCRSpJvfetbSZLPf/7zSZJPfvKTq157vdZLsmWD+/3d3/3dsA/5GPLKN77xjcO+t7zlLUnGPlelsZua9Vo3BpJZ3HrpyR1/5md+JklywgknJEm22WabYR99gPmDsWXDhg3DMXe+852TjFLyn/7pnx72XX/99UmSHXbYIUlyhzvcIcko20xG94GDDjpo5l4byyLXy02cn6Q/d7/iFa9IMs5D73nPezbq2vvvv3+S5PGPf3yS5M///M/XdH/Gv7UEAJxqvcDDH/7w4fM+++yTJNlvv/2SjLL+pz71qcMxuJJ973vfS5Lssccewz6CLR5//PEzx5x22mnDMVyT54yby9TrZVFZpnq55z3vmWQc+x/60IcmGeekZHQPY6yp88uOO+6YJDnqqKOSJCeffHKScZ7qcXNdydZrvST2GYP7iYiIiIiIiMhcFs7izyo9AXuuuuqqet8kYwCZutqFtRILz6c//ekkyeGHH77Jv+N6XS2jXuoK42/+5m8mSe5617smSS644IIkyfe///3hmP/+7/9OsjKwVZL82q/9WpIxwM/b3/72JMl//ud/ruk7kb6ElD4777xzkuTb3/72cMxll12WJPnRj36UZDa4H0Ebd9tttyRj+qB//Md/HI4hZd16rZdky65iYjFLknPPPTfJWG5VXfHqV796S32ldVs3ri5Pq14I7ooV9xvf+MawjzGM+QPVE5bIJPnZn/3ZmfNIFZeMlk+ujbqMcSsZgzVh2dl7771vzs+YXL2U85PMWqde+MIXJhnL8R3veMeK81qrfIXUWMxb++67b5LkcY973HDMm9/85lvytQemVi8owHgG+NrXvjbs41nqoosuSjKm7HvSk540HPOqV70qyVgH1XJ/xBFHJBkVNcxBNU3jlVdemWRMsfmrv/qrw74pWDCdX6ZVLyiQn/vc5yaZbcs8dxNAFDVLDeBHOlL6Ug2GyTMxwQA5FqVZknzhC19IknzgAx/o/aYki50uNrHPaPEXERERERERkbksnMWfdEhYV6p/S2uxqb8Na8Ftb3vbmWOrL/umYr2ulm299dY3JMnrXve6YRsr+F/5ylc4Jsls2eFnhM9qTW91xRVXJEme9axnJUl+4Rd+IcloiU/GssY/llX8ZExNgiUO/75quWlTBFa/J6w4tAP8qO5///sPx5x99tlJkqOPPnpd1kuyefoM5dZawajrJHnDG96QJHn+85+fJLnb3e427MMXbUuwXvuMq8uLVy+tZYM0Y0nyspe9LMkYI4b5IBmtkvQbVE/47NdtHFv997G8tHNqvQeWfhRW+GsmyZFHHrnaT1rBItbLPCgjxvPqS/7ABz4wyaj2q0o+WMtzTHuPpzzlKcM+rG5ve9vbkowKgmTjYpssYr20/eW3f/u3h33MD8cdd1ySMRViMqpVmI8pp6rWYx9/iXGRjPVB/8LKWRVpfCee+0466aRh3zOe8YzVftIKFrFeloGp1QsqScb5ao3H0k87h/p8y7zSoyrH6nk1HSDvMzzz/emf/unG/YD/n/VaL4l9Rou/iIiIiIiIiMxl9eWldQr+eaye1dUyVq57viytnx8r/Fijk9Enc6rgT49vUTJa2vGtx6LVKxdW66vFnWseffTRSZK3vvWtSWYt/occckiSsey/9KUvDfuoR3xnWb2s53MM960+/vg54T+FtaC2i6nX62rQ5ik/MjdgbUzG7AmU96/8yq+ser0tEd1f5JbSWn9rxhEsMFjq6RvJOG9gLWEMJLZJMlop8Xmu1nzOb/tH/Z9rfve7300yxlhJNs7iPwWq5b7GjUlm/cRf85rXdM+v9bwW/9XWYvbhD394+Pz7v//7ScZI20TMTvptZUq0ZYY/fTIq+si2U+dl+gltmbm4WiCvvvrqJGPZVX9m6hxFHvvoW0nyqEc9KskYZ4OsQ8noP01cIpFbg0c84hHDZ9opf7H8J2O/4Dm29w7DNsacqg7gPJ51mctqDBr6JJZ/MpkkyZlnnnlzfp5MEC3+IiIiIiIiIhPGF38RERERERGRCbNwUn8kLMgvaxCeeen8kPm1sraa0qdK/6bIWWedlSTZbrvthm0E6EH+j1SopvNDqop0vMqPCNh33/veN0k/5R4SI+T8BAlMRlcB4P417Q/1yP1rABSC+iG55R4V0vktG620FQlnlVLCu9/97iSzgdCQdeLiQb9S6i+LAG25BhujDdM3SL+UjO2aMYhxp8r5+cy8U92e2sB/zDVVysk9uH+Vab7kJS9JkrzlLW/ZuB+6oPRk+QSPe//73z9sQxI+z9VojSmqVr3Om970piRjMLsq9Z+qxH81dt999+EzZcbfOvci7UfOzLxSyxVZMkFjq9sM5yP1f+QjH5lkNvgvddX+TcYUwEr95dakPsMy1jMv1MDK9B2CY/N/PYbPSP3r3MF5HEM6zfo8XlN1J7PpYpX6C2jxFxEREREREZkwC2fxh3mr8L10P60VB2qwmKlb/OG8884bPh9wwAFJxlVDVhrryj7WKaz71Zp/1VVXJRkDl2CdYTW/nk+qwHptgl1xDIH4uFcyWuXYVy143Jf2QD1Xa3dVLywTbRo/UmBu2LBhxbFYWXbaaadh2z3ucY8ko8W/vd4yUceNjUmBygp8XXlv08ZhRa5tFlUMbb9anblmu7pfA6TRR+kzNVgndV2tmlOE1H2MUckY2BKLTC2z1prfU4lRroxlvXmoVcbUfkM7ou7qWPaqV70qyfQt/r1AfKQOZaz+/Oc/v+L4TaU04jrVekw7+NjHPpYk+Y3f+I1h39///d9vkvuud2iTNUUl26iDyy67bNh3r3vda+YY+ktVlLUpzKp1EmUBqj3UnLVdcD+21XGY4H4ityb1WZexv1XDtJ/rsRX6Xu85tqrLkjG4X30O4Bj6WU3PLAJa/EVEREREREQmzMJY/FkNhp71sbcq3NKupC3jqvE3vvGN4TPWP9L1tP7cyVhWWCFrSp42VR4rjXV1kmNYBcVSUPdhhbnoootW3APLDNaxunKKpaaN71CPWTYfzdUgZeNpp5226jEnnXTS8PnlL395kuRXf/VXk2ycpXsqrCVNGBb8ujrPeIXlslqdadv0FfpVtZS1Pq09P0C+U8+CSf9jW+3P9CP6+l/91V8lSd73vvet+hsXiSOOOCLJWJ61Xtq4F716bceLWvYoMHrW56pkSkbrTT22jZdSLf6oCH7pl34pSfLP//zPK+4xBSjzGo+F8iC96JagV4fHH398ktlnCFIIEyNnquy6665JZscRyujjH/94klmLfauIQTFT1Ulci201vs8LXvCCJMnnPve5JMmznvWsJMmxxx47HFNjAiSzfXEZn91Wo9YZzFPIMB/MU/F98pOfTJL86Z/+aZLZepE+PSs+UNat/399Vm1VfBXOaxWDVXHQfo9lVmnK6mjxFxEREREREZkwvviLiIiIiIiITJiFl/pXWRnyPAJZVZkLErU2UFAbMGPZ+PrXv55kDK5EoJ0qHWuD89Xgfueff36SZLfddpu5Hv9XkDTVFFZI/ZF9Uk9VMoWcn/qs7gXUP/InggJWafUyUcutlXk9+clPTpLstddeq57/0Y9+dPj8R3/0R0lGqb+ysRtp3UroH3vuuedwDGnBaIdV0k17ph3Thqu8GLk57bru66UrTWZluIxrHNPrM/Txl770pUnGvpwstqyZcQqpP/8nY1313CPYNk8iW4MBJrPzT5ueifqogRWpD+6Li1UyjqtTDcjUus0w1yT9gKM3dZ15zJPc9vZxTfpdDS5IANmps8MOOySZLV/GDfpQDfzHcW3Z1XmCbbiX4YaTjPPRE57whCSjm8Uf/uEfDscQFBUXge9+97vDvh133HGjf+NU2djAl/Pm8g9+8IMz/7/jHe9Ikhx88MHDtgsvvHDN9yJgcHXNwI1gCtSypH+Q4q8Gw2ROJg0fz8G1T7VzSJX+t88E/K3P0xzPs/ayjF23lHZO6bkAUm/3uc99kszOX6QUXZRU11r8RURERERERCbMwlj829VdLDd19R5VwOmnn55kDMqTjKtqbdq6ZQ/8xkpVawWuK1etlasGByT9EquZbVqmej4qgMsvv3zYxwoldccqWg2UxWocx9TVZlaTsahut912SZJzzz13tZ88aXor+Q960IOSjG3+4osvXvX8v/u7vxs+Y7Ve5kAxrPz2rITt6u4zn/nM4TOpEGmX22677bAPay+p5dpgfcmslTiZXZFuV6fbYD/ttZLZoHaMgZyHpblaDhaZI488Mkny9re/PUnyx3/8x8O+pzzlKUnGMqjjVKuuoAx7gRWh1kUb3K93DOPdHnvskST527/922Hf7/3e7yVZmfZpKrRtknEpGZVaV1xxxYrz2uCZawkyOs/ysharTK0Dgi3+9V//9U2et8gwP/fGEVLv1bJvnwt6AeaoO9RQNXjs/vvvP3Msln8CDifj8wT3rffEAifzoV5qu+e56ZhjjkkyGyCOMmZsRMX5rne9azjmF3/xF5Mk55xzzqr3/cd//MckyUMe8pAko3JwalSrOs/BL37xi5PMKod4/uIYxrOq1ON9pPfMxTaeg6+++uoV5z/60Y9OMi1FxeZiXopmAmFXFSeKSN5/anDahz3sYUnG96mq9Gih7/EMkIzKpqOOOirJ+PxYU69vSrT4i4iIiIiIiEyYhbH4s7oFrJ7VFWCsKe9973uTzK4I41/EKiarPW26mGWDlF74w7KqWFcRsVb2/I4oY6zwrILV2Alck9Wwalnkvuxj9bRa9TmGfTXewzXXXJNk1tc2mfUFXAYe/vCHJ0le+MIXDttOPvnkJMnjHve4JMkJJ5yQZFytT0ZLG+VX+8wXvvCFJGOaN5Q0+DglyZe//OUkyRvf+MZN9VPWJXV1uLUYYpV65CMfOWzDYojlvvYHVvV7Vqy1gBWt9VOv1oF2BbuXnpPr8H1YtU6SSy65ZKO+03rk0ksvTTJabJNkv/32SzKmJzv77LOHfa1lbJ7CpRdngeMpT/pUTdNIKlMsqFjTlgnm7uc///nDtq997WtJRj9zLB/J2voH/QvLJfNakuyzzz5JxvpBVYBVp/Irv/IrSZLrrrtu2Eb9ocggvdnUwMrUG+tIY1nHkXb8ob3XYyg7xhquk4xzDeMRyqOqHMA6Rhuo41pVUclIq1iinpjrk+T1r3/9zDmkUU7GdsA4Rn3U888444wkyamnnppkVlWF1fkZz3hGkrEN8KwwFSiX+qzM+MMz7imnnDLse8QjHpFkHFsYB6tSjM891RfP3ddff32ScTyramieQXjWq32R/rmxzxtTZZ5yjPdF2ncyG6cpmbXqUzf0EdRTVTEAT3/605PM9jnGshe96EVJxrrV4i8iIiIiIiIiG83CWPxbX6LWwlt585vfnCQ5/PDDh234+7erXViFlh1W3VnVqj5frD5iAa60EbSJYl4jlbJyTFnXa7d+T9yr+iRT16ys1pVnVspYPcPSX4+p1p+pcsghhySZXWFk1ZK6QTXz67/+68MxrSWnrmqyas0x9KFafw9+8IM34a9Yv/QsXZQjK79/+Zd/ORxDRgTadV1dbqNgQ7V0cb9eDBL6BmqC1jJTP3OdqlJo/Qepz69+9avDMVOI59Dzk0Th1asXymheTIt2X60ftvVUU0BfqnFO5n3fKcIcXPsUfQhVV1UDMJZjoccyjNorGeuOtlzn+TbLwvbbb59k9KtMxr7E96i+uVhzanaIKUI2iZ5yCMt7VdsxL7fWydon2j5VoT6o8152E+ipmqbOxoxD9Xm4fcZ92tOeliT5i7/4i2Eb7Z1nsjo/YJGmL9EGqjqWOqcvHXfcccO+M888M8mYGYb++/73v384BlXAIsOza23blBnb6vxCmTGe9Hz8Ob8+P7egCuA61Z+f2ALtPdrPMo5pyUrFGP2itfJXat2iJKfv/fIv/3KSWXUzbYI4J/V5CwXNlkKLv4iIiIiIiMiE8cVfREREREREZMIsjNQf2sBaNVhMm47qPe95z/D5UY96VJKVabJqarplBmnRwQcfnGQ2GBluFshgfvCDHwz72uA/nFdlx0ic2FeDmbTBz5BxVvlTe16VsvF9keSQIq3KMpdB6o+LQ5VzIVslYFab+jAZ5bKUca13ApER4IRyrMH9cO2YOj35KUF2CAhW2xyyLtpqHafY16a/6t2D/lFTBnFeGyRonpy2ykX5TnzfO9zhDklmA6geccQRK641BZBAUlZ1LOoF7Kvb6+dW1l9ZLd1i3YeLUp1/1pKmbgoQRBKZcDKmYWX8r3MMZd0GbK1zOZ9JB1sDKnJtwC3s05/+9LCtdXWrMlC+yzz57RSgnGo7pOxxA2CsSMZ5nXGEcaWmuaJ/8Le6BdQxMZmtc6CN9FIwVpezRaU35vBb540Hrfy/F7CN9Iik4XvHO94x7OPa1FV14WifkXtB6Ohf9NfqMkvQRuqHOejQQw8djtlll11W/W2LQm88YJ7mOaoGCW3TklL3NVg2bjK9+YX7tZL9npy/J/XnO9WxcerUcZw+wjPrE5/4xGEf8nvcWT/72c/erPu1rpnVnewrX/lKkuSd73znzbr2pkSLv4iIiIiIiMiEWTiLf5tyqa66XXzxxTPHHn/88aueD1r8b2SvvfZKMq6QfeITnxj2tRb3ulLPiiKr9ZRvXR1m9ZFj6qoY57Max2pzXeFvA9BVCz6rd6yssYL9H//xH2v63VMBiwyp5ZLkyCOPTDL2A+qh1k1rOahKizZ9DJb+f//3fx+OITXWVOkF6QFWbv/lX/5l5v8kuf/9759kVCH1Vt65NvvqMZxHvR577LHDPoLCvPa1r02SnH/++UlmLTVcu6fAoW/RDjZs2JAk+bd/+7fhmJ76YApQj6TJ6dVra3HrWRtvLvPu26bhmip/8Ad/kGQ2IBntFZURKfiS5Morr0wyG8wvmbVSkoIMa1ZN9cY+4DmhKlzob895znOSjME5k9FqN/U0WKQFq3MCn7FkVuskirJdd901yfgsVs9nrmYOufvd7z7sa58VSOFbrZx77LFHkpXphuv5jHG9YJrrnXnBCtcyDqC2qJbFxz/+8UmSww47LEnygQ98IMmsWqMNpNirc+qFsq/P2pzPs1mdL6hrQBVQnxur2mdR4Vmzztv0Icqgl5avPn8ls2XfqgF6qgKO59qUbzL2Ac5j7EzGPrzoFv95zyZtn6nPPW25f+pTnxo+My/z/sNzdC/d7lruT7DLJz/5ycO+9n20XmdLz/la/EVEREREREQmzMJZ/IGVrepXe8opp8wcg7/fPJbFR7lSV81ZPSR90YMe9KAks5ZcUsRRVviMJ2MZ43/ZS7kHWM7qCudqPoB1VRLLDnVdV1g5Hr8yrDjLlrqEeqir6q0FpLf6PI925R7/12o5aBU0U4OV2HnWPlaFX/jCFw7b8HkkBVIdp7hWL+YFYFnDErnffvsN+1iVpj9ifanWgdb6VtsC+1De0Gaq1QjL3CJbOXspwFqrel1153PPbx/affXa3I9rz1OLzLP4Tx3mmOpHiVX+vve9b5LZsQffSNon7beq9bBsMVfUesLHmfkHZUy1PhPHhrSoxIpJxnGzN6dNiaqgaOn56L/oRS9KMir5iJlQ2zF9AutoVftxTZ41fvd3fzdJctRRRw3H0A7mxVeg7aAYWESe/vSnD5/vec97JlmpaKhxXlCCUWe9urvggguSjO22Phu0/uZV1dSqEKizGv+H+jj77LOTzKpnmDPoQ3zXqtREJbLIUK6951nStfaeQ9vyrX2q9eOfpwagXup7Ds9oKA9qvfAMsujvPBtjHa9xQ6r6IemrVXnXof4e85jHDPtQQa/l/tTD0UcfPWxrFWu3prJPi7+IiIiIiIjIhPHFX0RERERERGTCLKzUv02rlCRf+tKXZo6pMheoAYWS2cAYy0IvoAzBKJCD7b777sM+yghpU5V6I5dBBoY0uUqEuR/SlpqGh/pAesb/VXr2/e9/f+a+VfbH/ZDFfehDH1r1d08Z+sPXvva1YVubxor6q/KnVl5c2wZ1QRnjTlHlmq18aWpQBm2algrpDn/rt35r2IbUC3kdbThZKbWn7VZ5chsAsAbVQqKMOwz3qHXXpsaq16Yfcn/O/+u//uvhmKm6yqzFNYXfzpjUcwfoyfI5nrGrN0dtjoCBiwZuMHWsItgXEkskxMkY8I8ATEiHkbMmK8eqOscg30R6jHy8tvGHP/zhSZITTjghyWygTIKa/tM//dPG/dAFg/m1pk1krKJcq1z7Fa94RZLk//2//5dkbNt17GHc4jqt+1i9JineqstRG7y33p/+hevhIkr9n/vc5yYZ3SaSlW577XidjOMYcwD9JhnLHPcKXAR6bluMTXV+Yu7ivnyP+txFH+b5rwbQ5Lj2vOpu1gbcXESYo3tyfGTlPfdKypcxqrpQtOl367zP/EI7oFyrWxL3Z1tNJ8hzAy4giwq/o87ltf1XSPedjGXzxS9+MUnyyle+cthHwFnqjYCYD33oQzfqu7Uulrh6Jsluu+2WpB+UsX3Oo+/WfnLiiSdu1HeZhxZ/ERERERERkQmzcBZ/VhhZ/aqrmG1wvwopr1orKKtvciOsPNVAMignsIL0Ur+0Vsue5bgXjIRrtoFSetZL/tZ9XAtrxdQt0KtB2dayqVarZLQyVmvjvAAjrVWzl5KJQHNTpWchpo2T8ouUUw94wAOGY7Cg0I96AebmWdU5nmN7KTD52wtU1wbNrBZ/rAqMofz/+te/ftXvMxXasuoF8mst/bVcqRfO66VJbOml7Zl6yr4eqMiwHlfLE0H9mCvq2MXcwF+sO/V86oM0mtVSwryBtRhLW+1TWGHYh6ogGce9Jz7xiUmmmyqW316t8liweAbYeeedh320YaxjlH19pmJsqVZNoO8wd3/sYx9Lkuy7777DMVj8sdz1rNY1deOiQbmQUjUZ1S8E7MOqX8eaVrVZxx7KiH6GZbE+02FZpM5rcEDqvLUoVyUIfYfgZ1VJy9zH+aTPrEGh3/zmN2fRoQzrPM44hKW3jvOUB/ML9dOz6vO3zk+UOWMViqdq1afvUp/V4twLIryIMA4feOCBw7Z2XCDwKyrVZJw32FbT+R1++OFJxjp65CMfueIeD3vYw5KMdVrnCOqQOYp0tDWQImMn/bH2Z85rnxfrPaoi85aixV9ERERERERkwizcEhArKKQDwdcoGX03ehxzzDFJRj9cVn9YPUqS973vfZv2yy4ArYUeRcVee+01HMNqchsfocJqYs9CyvkcU9Micd+avq+l9XGrvmbtyncvfsEywCpiXZVv0/n1VDLzyqutS1aR6yomvkhTpdcu3/nOdyZJ9t9//yTjKu0XvvCF4Rj8uegz1YeyWl6SlSnm6mdWkuv3oI+21ut5dVktqK3VGpbB75yy6sVVWE0NMO+YnjV/LWn5ltHiz5yNhReLV7IyVV5V5tEHqAfUAT0LFtepPv5YwR7xiEckSU477bRVz6Nv1LnuPe95T5LZlJxTgrJmXKr+svhp85xVnwtaP2bqo1r3mWuojzo/0QewwJFSsSqnOL7tt/X8RZ6DSF1YUxgSU4KyfshDHpJknFOSUWnXS+VKWbfzf+1T7MNaXfsiZY61mFTPWO7r+e9973uTJJdddtmwj2uhvkQBUuMz1PlwUaFtV4s7Y1Qdf1ooO6y5dd5mXyUnBCYAACAASURBVFt39ZrzUsrSF1Fp1H1Tmd/pK1V5RZ+hXzCe12ddlEEoimqfoR/RVinHOpdTJ+w744wzhn08f/OXZ7Rzzz13OIZ4Aa9+9atnjk3GPseYypxFXJtkjD9zxBFH5JaixV9ERERERERkwiycxZ+VHVZf1mo5wRe8teIQMVhuBF+zutLFahh/e77JlCfH9Hxn2Vatj219sOLWs+aweln3sbLHSll73XrelOE3Vp+mtkx6VpN5tJZLVrar1WYt1s0p8Ja3vGX4/HM/93NJkq985StJRgvmfvvtNxzDNlZuazlRjj0/caA+qcMawfxBD3rQzLGoAWq/aDNp1HESSwzf8atf/WrvJy88PQvHvGwHPeVF0h8/etZ96pH7zssAsIzgd49yqJY9bZeyrtb1tlzbjBf1GKwx1dLDfEO7py9VCySqArZVVQ4Kg6nOI1jCem2askblhZ9rkpx88slJxjmH+qk+y6gHyLhEWSajHy51TcyG3tzPd6v7qOMpRIivZY7ygb/EUJhHfaZCAUG9Yl2vFsYaZ2lzwzNaVRy0sbYWEd5BakwM+sJHPvKRFcczXtGnWtVFMlqhe8/YbOM6bVaeZBz/2mwcyXR8/KGqIrC+Vyv8eoPxsqrMby20+IuIiIiIiIhMGF/8RURERERERCbMwmk/kCshd5mXEqtSJS/J2uXOywYy7p4LBXKyVi6brJT699LuIJWsUnEkSciO5wUHhFrnyKZq4JllhGAuNajMatLUup1625hgY4cccsjw+fTTT9+o77monHjiicNn0sAg3ULqV1MUIWVEXlfLF2lemzapF7iKcatKBwli2qYHqnCt3j7qnH7493//972fvPBQBr30oz35fS8V6WrHzOsvjIEcU8evZZb9P/axj00yyuh7bROZeJVxtkFl+b83jiErrjJ+gmgRcIl5rI6VzFdIdWtfZBspgacGv2+e+wqy4pqS7eijj04ySspxOar1whiFHL8+F1CPzP3M5dX1iIB2vQCmfKfq3raorGU86T0bMZfUZyLmI+qDPlWDLlKP9MW6j7JmG3NQrTvmN+5R08bRrzieeqrpM1s3xEWmuhxRnp/4xCeSzKY+btPw9gIBtjL+XspR+lTPVZBnkoMOOmjmXu1nWW60+IuIiIiIiIhMmIWz+BMQZpdddkmy9iAhpFVo0zTUNCQyru72AmOx2l6tMe1xWHEo5wor170APQRe6qUMbLfV+2PpriqCZPmC+/F7q7KlLTdWhqt1oQ3419sHrGZXa1pP/TFF3vWud93kMb/8y7+8Bb6J3BLmBdxrt/UUAPQTjq1WFI7DItOz4i2zxR9LO+nBalCqdoyuqZY4r1UI1LJkH/NRrRfqkaBnzB/VgomVlOeJev7222+fJPn4xz9+k79xEeH39dQsbVrRv/qrvxr2EYi3VetV63Frnax1SL0yZ/H/kUceORzza7/2a0nGsbWeT13zDDBVaKc9FWUv7VtLDerXggpmY6lpypYV+kYdq+DCCy9M0g9yTRvmvGrVb1Pt9lRNbfrMOg6effbZSUYVQd03L2W2LBda/EVEREREREQmzMJZ/FsL5Vp9hbAIt5ZNUsgsK6tZw+sqMauHlOE8/33+Vr8jVuY5r2cJW0uqkXY1NBlXvOuqaT12WcDqUv3F2rgWvfLrpXuDVkWBpb+ma9qSaYFEbimtH3PPb7a10vSsNjBvnOupC3rKmmXhb//2b5Mkr33ta5OMlv8kefSjH51kZeq+ZFQVMe6wr3cMdVXLnG0cj2W5HoOFn211jKOupmrlbH3ke236bW97W5LZdHzEPOAYLJFVadaqzuo8zdyNkgwV5z777DMcQ3wGqGoC6mxqacpkMUDxcsABBwzbsOKjpKhtuX1X4fmsjjXQi2HW+ujTf2oqR47hmvMUnLK8aPEXERERERERmTC++IuIiIiIiIhMmIXTSBGgAsnXWlPsIMtB7oIEZs899xyOOfPMMzfZ91x0CKKYJPe6172SJN///veTzA8og0SpymNbqWyV4SMTpD7mpVlEQkvKpnr8skvOe+kQb3/7288cQxlXaSR1wbbq4oFM7ZWvfGWS5B/+4R+SzErLaroakfUOcuTeONXKzJFN9saknlS/DY7WSycIvdRKU5f/n3TSSUmSY489Nknykpe8ZNjH+E39EIgvWSnVpzyrjLwt4/o/bgBsa10GklFujvy2lzZuqu5jbZC9Wq4EKeP56cUvfvGwj7SmPFNdd911SWbdJ3EXow7r3HHYYYfNbKNe6vx03HHHJVnpylevKXJrwPhR3U9a90qenZOxveIO0BtPcNXkGbu6Aaw2DlV3GFL90l/rGDf1+UXWjiOniIiIiIiIyIRZOIs/ad9YZa7BzObBKjSWhWo1lpXpQ6699tphH6oIVg9r0DcsV21wv95qZs/KBlybfTWAIPuwoFXFAZ9bC9pUrTOrcdVVVyVJ7nnPew7bVlNa9Mq2Z4HcddddkyRPe9rTkiRvf/vbk8ym8HMVWRaJdryq7bdVH80LFMe2XtrTNqhfVQxw7Z5qalmCL73pTW9KMlv2zOuf+cxnkiQ77LDDsI+5uk0VW1Pxsq0XeLatI+av3piHxa5eh6B+vbqeArvvvnuS8dmotkPmANQaJ5xwwrAPRRl9CfUXdZmMQc6wclYLJuXPX+aw8847bzjm/e9/f5LkrW99a5J+KuA654lsKVCqzLP43+c+9xk+kzYTZUwvOGW9VjI7T7Tp/Di2vgOdeOKJM+fttddewz7UACJa/EVEREREREQmzMJZ/PFdmWc5mQerxKyaHXPMMZvw2y0urcUff/5kXNkknVu1puOL2VqXeyv7a7HC48dUfQGB8/fee+9h2znnnHOT11wGrr766iSzVqnnPOc5SZLPfvazSWb999fCRz7ykSTJk5/85CSj5b9aw3q+lyLrlZ5FGJhTWstuLyYGVKsx+7Bu9pRNHNOzNi8LWPM3bNgwbMOqjvW5jitYtKgfyvDggw8ejrn00kuT9C3/HN+qm2o98/mud71rkjFmSjJam6fK8ccfnyR50YtelGR2njj99NNnjn35y19+s+5B3IBaL0cdddSaz/+TP/mTJMnjHve4YRtKkJe+9KU36zuJ3BJQPtY4YVWFlCSnnnrq8Jlxr51D6jzB3AF1HGxT/DHW9eaSyy+/PEly0EEHDdtQ7Yho8RcRERERERGZMAtn8cc/BoswUf7XCoqBnXbaKcmsr//GWkSnzMUXXzx8PuWUU5L0VxhZ9ZwXjX8tzPNvxdJDXROBOTGaP2y77bZJZuvt8MMPTzJa7FFxUI9Jcpe73CXJ6JtW+xN1Sn2zso11LRktZCLrjZ7CCMsjFt2a+QL1EscwJlWLP5+xEPcUL5yPz3IdG7F4ttdJlideBn7fWJqTcR4hFk+NLM9n6oqMM9UfFuUfKoJeHBLGNuqgHsM22sBpp5027EONMFXwC37HO96RZLa9b4xVfh7zlDYttb/QPz7/+c8nmbWu8ix49tlnb4qvKLJRMI7tu+++wzbiIMHJJ5+8Rb8T0F8OOOCAYRuqUBEt/iIiIiIiIiITxhd/ERERERERkQmzcFL/V77ylUlGSfPRRx+9Uee/7GUvSzLKc5T330gbIKSmJUEKuF6o6X7kRugPVaaJFA1JK5LX6lZB6qVeqsRtttlm5jwCPW299dbDMcrHZL3Sk87j6vXud787yez4f/311ycZXQRqXwK2IfGv42bv+GQ20CkuBlVmDsuWgvQ1r3nN8BlJKmNODa6HOxH7PvrRjyZJPvnJT26Jrzl5GPuvvPLKJLMBxmrq3lsCc85a3Fl6aROPPfbYJLOpy7jmsvUbWR8whtNvkpVtsee2sqmpz3P0L54HdcWUHlr8RURERERERCbMVssSUEhERERERERkGdHiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCbMbTbm4Guu+e4Nm+uLLCvbbbf1Vrf0GtbLpsd6WZ9YL+sT62V9Yr2sX6yb9Yn1sj6xXtYn1sv6ZF69aPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwmx1ww033NrfQUREREREREQ2E1r8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJc5vNcdGtttrqhs1x3SR59rOfnSR5yEMekiS54YbxVre73e2SJN/+9rdn/k+SH/zgB3y3JMn//M//JEn+7M/+bDiGbbeUG264YatNcqFNzKaqlwMPPHD4fM973jNJsvPOOydJfuZnfiZJcuWVVw7HXHvttUmS//u//0uSbLPNNsO+PfbYI0nyEz9x4xrU17/+9STJZZddtuL8M844g98x7Kv1f1Os13pJNk/d7LDDDkmSrbfeOknywx/+MEny3//938MxP/mTP5lkLH/+r9vucIc7JEm+853vJEm+9a1vDcdQTxdccMEt+t7rtW4251i2CCxTvdDO99prryTjmHbb2952OOb6669Pkvz4xz9Oktz+9rcf9tHPLr/88iTJueeeO/P/pmSZ6mU1qKckedjDHpYkucc97pEkOeecc4Z9n/vc55Ikl1xyyWb/TstUL3e84x2TJDvttFOS5KCDDkqS3Pve9x6O+eY3v5kk+dGPfpQkuetd7zrsY645/vjjkyRf+MIXkiT/+7//u6m/6lLVyyKxjPXCGLXnnnsmSR7+8IcP+3ie5W+dX84///wkyWc/+9kk4/yyOViv9ZLYZzZF3WjxFxEREREREZkwm8Xivzl5wQtekCR58IMfnGS0MCfj6hiW5f/6r/8a9mHtvOaaa5KM1v23vvWtwzGbyuI/Bfbdd9/h81lnnZUk+ZM/+ZMk44plktz5znee+cs+rGVJcpvbrN7MWN3HQoN17Pvf//5wDFaDs88+O0ny1a9+ddh36aWXzpy/rDzzmc9Mkuy///7DNiwyd7rTnZKMSguUAMlo1Yef+qmfGj5/73vfSzJayrD0X3HFFcMx7MOic/HFF9/CXyKy+aGf3O9+9xu2Yam8z33ukyS5733vm2RWOYbFH9VRnX+wYDImbdiwIcloqUlGq+ZnPvOZJJvHujl1mJsOO+ywYRvleNpppyWZrVfqiLKv84esDeb1arHH0r/ffvslSZ70pCclSR7wgAcMx/AsRt+oFsyrrroqSbLtttsmSbbffvsko+ovGeeT6667bhP9EpFbh9133z3J+O7y1Kc+NUnyxCc+cTiGfsLf+jyG4hVlGQq1008/fXN+bZkgWvxFREREREREJowv/iIiIiIiIiITZuGk/sjLkIkhJUtGWQzS8u9+97vDPiTNBPlD/lcl6VxTRtlekrzuda9LMsq5KwTew4WCoH4XXXTRcAxljOS87kMOS0BGJE7I1JPRjYAgKEimklHmRH3ilrAsElr6A7+3urdQ3vQRgvMRmCwZy5lyry4WHE+998qUa81z5xC5NaEfJMnznve8JMnee++dJLnXve417LvLXe4yczxy/tpfaOfMI7iQJWM/Q7LMdeocg4vBox71qCTJv//7vw/7lt1d6aZAbk75VpejU089NUly3nnnJZkNYPqzP/uzSUZXJ8bMGkBW+hx66KFJkh133DHJWHbJ6DLG/IxEvwZ/5Zif/umfXrGPuZ+5h/kdV5skufDCC5OM7jMnn3zyLf9RIluIXXbZZfj8oAc9KMkYBJP5BQl/MgbL5nm2BoYlgPJuu+2WZJyX6nMZz78i89DiLyIiIiIiIjJhFsZMR0AfrDIEWWqDkyXjClhN+wakyUAdsM8++wz7CP6zzLzwhS9MkhxwwAHDNiy+WLmqBYyAiNQDFuMaKJFtrOxj3a/7uCZ/q8WG4FpY1GpwOtoF11m2QCeUO229Wt6xxFBuWCex5CcrrfhVMdDuo8/UgDOgxV/WK89//vOHzwSEo29UZVFN25eMY1EvfSjtvTf/0M8YtwheloyBmZjHsEYnyZFHHplkNhWqjFCOlD1qs2S09AMp/JLRak09Uvda/PsQrC9JHvrQhyYZlTE1mCVtn7mev1dfffVwDPMJc3+d1+s8VI/Zddddh23cF0VhReu/rFce97jHJRkDxCZj8FgCZDJPVKVx+zxdn5VRMBMgE+VrnbdQDHzyk59MMtvfRECLv4iIiIiIiMiEWRgzHX4trISxklUtjVg2a/olwHqJ1Zr/8ReUG9lzzz2TjJapZLRyUb6UYQ/21Xqhrqi76hfLZ6xrrPpXazMrmm0Mh2S0At397ndPMq60fvzjH5//QydCa82n/JKxvDimVVXUz5Qxdd2D61UrJ1a0ZYmpIIsDaqAnPOEJwzZ8laG2ZT7TX3rW/FZZU/sS++iDvfPxdSYVU0079+xnPztJ8jd/8zc38cuWC8qK8YcxqvqLt1TlUjvnU2d1jqvxgJadBz7wgcPnQw45JElfLYYSsM7nyewcRJ/gb1UCUh/UJ8fUuE2oM+jL1YL5pS99KclsXBqRW5M2rXJVi/GZNky776UdZ+6oY9xqz930kXpflGzHHHPMzf0pMmG0+IuIiIiIiIhMmIWx+Fe/7krrl5mM1pjqv8mqMCtqrCrja7nsEO36bne7W5JZXz5W5imzanHHesLqZc9i3FrQ6oo+tPVY74EFAQsDFqBkrGPqEf+nGkm7Rn+eGsS6oIyqzzBl0Vq6anYGyr3ns9xa+Pm/lj/bWquPyK0NlsvqMwytlTEZ23mrjKljURtTo55Pv+La9K1qAQWuWa01WFcZy6677rqb/I3LQBtThDKvvuTzwKLWjlXGJZmFLBQ1qj7zK1H1exb7qnpJ+u0dpUu12NNf6FNt/6nHcz7Kz2T0mT7zzDPX8vNENgt1fEKZQl/q9YXvfe97M//X9s6zMXMHz3d1H9fkvKpw5jP9hOf5ZO3jpUwfLf4iIiIiIiIiE8YXfxEREREREZEJszBatyphrlTZeCuvqfJxZGTIbJDJICFbVh7zmMckSbbZZpsks9JTQNpNWVcJKrJJ5Jet7C9ZKaGt8qdeIMZkVoaJlArZYZVWtTJ05O0bNmzoXndq0I5xzagBq2jbrTy5SsvafbXPUAdt0MzqQsN9e5I22XTUIKTUAzJY5Lc1QNm8II3LwoEHHphk1jWlDYZZ5w/miLWkg22vV/cRhIl9vfGKftNLxYRc9KSTTrrJ37gMUH6MZ9TLWtMe0k8YG1v3JrkRUufVsYaypg5qILI20GWvT/GZMarXX+gTzCHzXGNqaky+r1J/uTWp7idI6xmrqmtMdbFM+s/KzB30pRrQrw3A3KaWTca+RMrA+t2U+gto8RcRERERERGZMAuz5N2mJmP1rK4Os/J17rnnJkn222+/FfuA1eplD0pGuWKBalcck3H1EtVFDU6CNYXVy9aCnIwWtF5wP+qvDahVg8y11rGeNbO970EHHTTsu/DCC/s/fgJQlm3Kq2Rs8/SV1jJT91GmtT9R3vOCN3HNZbKeoYqhze2xxx5JZgOFkl6StlvbM2VO3aG2qav79Bn6HMckYxsn1Q9/a6otLHNt4Kz2czK2h4suumjYtsh9Zvfdd0+S7LTTTklmxwvaKWVYLZhtvfTaf3tML31mm6K0ZwFtrdfJqKTZe++9k2jxB8oRpQt1Vq1p8/j2t7+dZLRk0ydUxczC+FVVeJQxY14NztemF+spyugLvb5EX2jT+NX7o9Jo+2T9viK3JljXk1Fxyrxf23ub8pi5pDc394KWA32hDSJb7w+mK5ceWvxFREREREREJszCmOlaS03rF5aMFpf3vOc9SZK99tpr2MdKGMdjXamWuGUBi1iSHHvssUmS+93vfkn65cGKIjEUvvnNbw77sL606bF6vuJQrVycR92x4llXLrF2UnfVl7m1wEGN3YBFdIpQlvzeno9/a/GvtIqXnhqjTUvWs1JOPVYG/tfJmNrqKU95SpLRl7yWPVYAyqWu9rcWrp7lkWthaevFxUB5c9VVVyUZU5Ymo6WO82p/4n7UPRbR8847bzhmkS3+KDBQS1TLJNvoE7X9087bMaxaZFr/yl4cAMYyzq9jIRYg6qNaUKkrUpEy7lE/ywrzDla0a665ZqPOp4ypF65X+yR9aa0qgilCyuQaE4PyYDyrChnKr/Xtr2NV+7zWiy/DeMZ1enM/36P2Vy3+mweUsqR1POecc5IkX/7yl2+177Se2XHHHYfPjPX0ofrMxTzUqix7z2X0jbqvVVdynRqXi/vST7fbbrub96Nk0izfW6+IiIiIiIjIEuGLv4iIiIiIiMiEWRip/2oplmowM6QzH//4x5MkRxxxxLAPqRpSS+QyyyijrNLTww47LMlKSXAvyBvy4xpoDIlkK/PruQz0gvu15/G3F+AHajpBvhvtoJUWJrMy7anRpkhExpqMZYgEvBfAD3pSzDYQGfdCclv3TT1QVk0jhRSSdsXYUiWypPXpBVSkjbay4ipxJSgP9619AFcX+h51XqX+rQtNpQ2ihStM/Y7VHWhRoA0jWe799lZC2QZDSkZJZjtX1G3UXR1nVnN3qmMZ1+a7VbclQMJMu1rGOaq291bqv7EBeZHEUldcr/Y/JLG4BVR3tqmPbUBA0tpeKWvm/Fr2jD+Ub+vCVLdRhtVtpg3Y17qt1e9EKrLrr79+2KfU/5aDTPxJT3rSsO2QQw5JMtb5rrvummR2rMPdrfYToH+RbpFxrLqPcf4UqAH0eLbiWaA+N/DcyhzScxNrA1723GKBvlTHSsYxyne1NOhy82HuruMPc/SiuIpp8RcRERERERGZMAtj8WdlsU37VlenL7vssiTJBRdckCS59tprh31Y4Fi15C+BsZaJnvURyxPKipqGBIsXK10EdKvb2gBybcqwpJ/qr1315P9qleZ+fO+68txaT7lvtfZNaXW5BQsVZVKt8a3FsxcQk/LqpVFkG22Cuq0WGSym1do8RbB6JKOVBIsIZV6D7NTPyWyAuVbVQrnW1XnGK47pWYaxKtA/qqWt7Ss96wJwj1122WXYtojB/ShXxoJvfOMbSWbLtVUi1f7SWid7KV/b/tJTL7XqmaoYoL+isqgWTD7zd+qW5qo8wzpIWVVVGcGzsLDQ3qvCpk0ZW+uZvkhdc159dqCN0M9qii7SA0+9PmqZA/VBmdVglK3armfVZ9u8QKataq0egzWz7RvJbB3JxsFY//znPz9JcvDBBw/76AvtOFiVlliyUX1U5dSee+6ZZGxP9L9qmZ7SM1ntN5QZY0v9zW1/aYNiJitVM3Uca1OZ9xRp1N3ll1++4v6y8dTyp123z2bJWCfM6zyD1BTLvMNyLMEzkzFANM+Jl1xySZLk4osvHo658sorb+nPGdDiLyIiIiIiIjJhFsbiz8piuwpZrZp1dSSZTfvTpiLjL75jy0S15gOrV+yraXtYmWQVq1oPWytyz2Lf1lm9f7vaX61A7f1b60H9zHmsPFcLKX6CU4ZyqBZMVoJXq6NkZaqrWv6rWY3rSifHTz0tZvXVox+wuo4Fsucv3lNZcFz7t1f2/K2pxyh/zqNeat23dVb7TBsjoBe7oaZCXRQoq6985StJxnqpFn/iMmCleehDHzrsa9MqMd7Utt2mIOu1+7bMa92jMMOKXOco5iK+/yKqLtYCiqEHPOABw7addtopyXxVGb7DWP5J25isVCVVaNfUOTEgajwU2g7KpTp/8H3POOOMNf/GRYQxrlrs23m5Kpfaub431q3ml5yM/aNNU1bv0VpQ6zjWKjfqebKSxz72scPnxz/+8UmSBz/4wUlm+wL1SLun/9V65ZkKC2cdB+lf+L7jd14VA1Oipxqjbdb23r57tHGt6vG9vrRabJPa7qkz+uvU0yxvaph3mI/qPEQ9957XmJPoF710iqgvGGerYo1ro4ThuaA+B+9gyAAAFyhJREFU921Kpv20LiIiIiIiIrLkLIzFH18JVkBYbakrIq3fED7/FVbSsHItk48/lq+qjMCy1/pv19UsVqPaiNbJyijZrR9S3ddG/62fW8tAXelso/3WVUz2sWLNqlpP7TFlqKNqjV/N6lvLmuPbrAj182q+z0nfwjYl9t9//yTJ2972tmHbz//8zydZmSWhZ9WnrOrKL6vIWEDaMS0ZrfGUb93X9if+Vv9bVpN7lgdWo/E5a8fW+r0XkbPOOitJcsUVVySZbaNYibH0Y/FKxr7A+NKzsFAPrbWyUq2SyazCCav+Bz7wgSTJ+eefP+wjJs1Uo/jT7h/1qEclSQ444IBhH3MT5dtTv1COZJyovrXUA3VYxzj6JVYc5oqqBGP8pN9hyaz3od+cd955a/3JCwFjFeVSy45tveetNp7CvHm2l62ntW72FIFsw6JWn9eoM9rD1772tfk/dMmgXp/73OcmmR3rmI/wv69qNcqVusdCWbMjHXTQQUlGX/I6VnJN2gzXq+PalCzRvbhK/L6qcmjjKPWefdnWm1dWU2DW+EqUOfN+LWfqs8bJWEaor7333nvYtlqMsvoeQdnSH3ptGJXLPe5xjySzcZM4HjUZfaduY25hLNtcKhkt/iIiIiIiIiITxhd/ERERERERkQmzMDpdZHZIMJBNVAlrm+4AqWeyUqZMQIya8m/qIEHdd999h21twLw2fU8ySotaOUwySvfY18oy6+decLn2mNZ1oH5H5Gg1VRoypzY12mqBUKZKT4ZPndLW2/R+9Xj+1iA9SJvasqySTo7fXEFIbm34rb/92789bEOi1absIe1Y3dZLk4jUjvrgb+0zbdC4KgekXlu3mlr3SJWpl+oGwFi6WnqgqYArQwWpMLLV3m9uUynVuuiVNbR11rs2fREp34YNG27iV0wHpPa77bZbklGinYz9BRlmndcps7bd1nRujPvMEfV86oV5o02DWe9Pf6mScvo5/X5qUn9cHnBpqHMvY1P7DJCMddWmUqztvnWHqtdu+1dP3sz9cAWpknS+G8HkllnqX4PzPfOZz0ySHHLIIUlGN4kqSWcbz9G9Om+ftWuQVFwFcJ2q7YLnBforYyXp0JJpBFzuyfLZxrNTdRnqPaPV7b1tdR/9pHUlqy4U7GM8q/2FOlsmqX91LcJtk3ZcXcVwE8eNGYl/fYagz3DN2p+Asm3do5Pk61//epLR1fqiiy4a9lGH55xzzkb8upuPFn8RERERERGRCbMwFn+CH7CKyGp9Xb1qgyJVBUBrqcFSPNUUIz3atHzJyrSIrbUrWRm0p5fCipUtzq+rmpQ99+itkPYsAoDFh+/fCyLECivX6/3GKdMqHpKx3NrgS72USj2rC8ez6lzPgzao49TgN9eVcyyWBHJh5bge01rIap+5293ulmQsX8atapVv66OuHNO2uW9PZUOdYY3rBcvkeI6p/aRaQ6cI439tt5RHa6mvfYJxpjdOtf2E/tdTa/TUCFOnDcCHVSoZrfdYSur4zdzfBsOqdddaj+s+LJY9Kxjwnajfat2kf/bOmwJYX1FC1PbKONAqJZOVFv7eswP05vf2uaBNKZyMgUextvXUhoyny8S9733vJMmhhx6aZAy2l4yBxSizXiBS6roNVpqsVPi141myMqBtnbta9SDzS1Xo7Lzzzmv7oesYLOg9tQTb6jsI7boNtlz7S8/639Kmma0B6FoFbA2SWse0qUNbe/SjHz1s43mN8byqVJlveB/kvbKORZRt71mZumS8pA/VAH6oyAjoS0rfZDZ97JZAi7+IiIiIiIjIhFkYiz+rjazMsLJSU1m0viukTkpWrnqyIlN9cKZOL+0YK4KtxaT6xnAMZV8tv601vfU/TlZax+oKJ+djTemtQLOaTJ3Vfa3fFH/rat4y+Ptj/a0rvNRXa2mr5UdboIxqihK2cc2evzoWuiml56nQLmt8BNoW4w3/99JQscpez6ce6tiVzMbVoM3P80GnfqnDeix1zjhZLQmMpRzD96/fp6fumBI9f0u20d5bFVSyMpZJb9W/HcN6api27pcByo72Wts74wd9o1r821SzrRKpHtNrtxzP+a0Pcz2/dx36UB1bp0RvjAJ8x2nv1bLbqr16SoxWnddTHlHn7EPVmYzWTCzc9bmE40mvtchUf+N99tknyahkwIJffYpJE8Zvr6oHyp/y6aX6ZQ5g7K99iXlknuqrjbfRU07RPtp7tb9lUenFxOB30barNR4FRmux76VH7qk02lgaUJXOzCuom+pz2RT6yVqh7d7nPvcZtjF28bcqWNr4ZdRVLVuOR4XUUx9Rt4ybdUxt54/eeLul0OIvIiIiIiIiMmF88RcRERERERGZMAsj9Udy0QYsq5K8NlBfTZeATKYNILMMMnAggESV27XBRHppRCgryr4np+QY5CxV1tJKxHvBf5DN9ALQIc3ppdJqAwu16Yd6v3FKUF694H6t+wPUsmkDzPTcKFq5ef0fSRXSKFKWTIVTTjklyWyQGOR8lH2bli8ZpV602Srtbl0DemNRK4+sddim0eq17zbdaZVOI1XjO33ve99LMusq1QteNyV6smS2tb+9N17OC8zUSjhrf+kFJl0WaN+9sYrPvTKjLbfzRi9oay9lbBtYrjcuzks7Ny8N7RRoXSDqOESZ82xV5bHtXNsLfNkG5K0BEtvAvr3nAp5ZemmG24Bai8yrXvWq4XMrIafM6tiBewbH1LJr3WZ641obYK7WC/NLW669wMHMK9UFhHbAfWknVd5fXRsWFeb4Wi+UEeVSpeIEA2zn9Prc0LqQ1TGHOm5d0urczhxOwM763abgXlEhXWWSXHLJJTP7SB1bAxryvEPZ1vKg/TLe8ExU+0UbRLkGqyQ9MKlFSe3cC+yLK08vpWUbEJPfkYzPopsCLf4iIiIiIiIiE2ZhzDqsnLVBeOoKMOkS4Bvf+MbwmZUUgl5MNf3YPO585zsnmV1hbMsBC2EvqFh7TDKu/LaBsOox7GvTMdVtnMfKW8+6wipc/W5turReerkpBypjVb9d3U/G390GFZlX/rWsWC1u20i1CLWpsqbG3nvvnWR25Zy2RrtmdbYGpeJ49tVypR/OU1tAzzLNCj+r0b02Tx31VDptsDOuU4M/Td3i31pPKvNSVPYUUdBahntl2AtMtyy06qzKvPpo54/e3NCqynr9tU0zVq/TpqHtBW6aqlqjTYVbace4npKCcaRXv216szoXUdeMVW3wx2R8HmhVBcnKcXAROfDAA5PMBr6jXPhdWMdroLZqbUxmrZdY6tsgf7VcW6VgnbuoD563eqpY6oq/NR1Zq2RjX31um0I607bdJ2N7p93Wcm1T8/bGk3mqop7KOZkdM1Hm9OaZqQRgfupTn5okedaznjVso63yu3tpWXfdddck/TImCGObVq/OA5Q/So+qiOE+BFDsBdKm/HvzWBtkvWfx/53f+Z0V3/vmosVfREREREREZMIsjFkHi39roaxWKlZ9AF/WZFx5m8qq182BVaW6UoViok1D0vMDZ6VxXjq4NrVP3db6fvWu2VtdblfPepa01opTVQ1T9c1MxrLsWfzblfeer2pbXj01SGvhrtBuqvJmSrASfMUVVwzbSC3V+qTWsYVy4W/PZ7hNhVQtAPTRXn9s70G99FLEtSn/6ja+L/2yfv8pq2SSvmW3p66ox/bO653fHlu3TzUl3FporTG1TdL2WmtnPX4ttH7Fyco666kC2hSO1WcZen1wCqBA6lmimFewQNVxvrVY9lQbbKM+q2W7TavVzmXJOA61MRjqtVpVwSKNXeeee26S2TLH93f77bdPMlr863MbKf7YVve1KY159q3W51apWdNaM49wflvOyUq1RrV6c23GUa5X+8+GDRtWlMWiQTut7Y1xhLLu+ejPs9jDPHUTtM/VyejjP0+1tuh8+tOfTjIb2wBLO772KFDrcxspL3luqu2R9tuO//WdkrZ+6aWXzvyfrJ4uvo6X97vf/ZL0FWTcn/qmz1Qlz6mnnrqiLG4u05zJRERERERERCTJAln8obVertUvlVVLVkan6q83jzbqdLIyom6rqKjMs5RQL9yj579C/fT82eZFy26pK8+tFQnLQl0FnbK/MiuKrHD2VojbCLE9S1tPjdEe35Z1vTZR/acGFv/PfOYzw7Y2Oiz+XdXq0WapqLQ+mG2mkmS0CrQryPU8VrzxS6sRt7kvK9e9LBusZnP+BRdcsOJ3T5U2PkKyui94byzq+Yn3so6057dxFXqW5anSxo2pVso2RkxvjGnnit4cs1omkmRlPfb6JvdaFsVYMrbJni849NREbVyE1mpVr0391Lm/tfRDVXug2qRvVQsa+9iGcmGR/McZg7/4xS8O22h7rTW/jlWtkqz2JeqP+qiWfmifh3u0fak3P/G3WibbLAK92A21HSwq/OaqlKPt0jZ7z9HteNJT0K4WX6meT33UPsFczntRrZepqDJRpxx33HE363zUAbU/tcoiynFTZn1rnxPrHNc+A6Lc6GWe2RRo8RcRERERERGZML74i4iIiIiIiEyYhdNAI73oBTObJ2Vpgy9MXb7Xoxf8pk2T0/5NVsqNeqmS2gBMPdnzvCAmrYyzdw/kYVW+1AZB6QVK6wWlmwpI9WqZAHVCX6EeqmyzLbdeIK156Z7aFCdT5Zxzzhk+X3nllUlGOR8pdNiejK4PyACrrIxxirJDilllZdQR26oknM/0lTYVTL1fz70HGT/pTs8444wkyYUXXjgcM4XgS/NAklnLpS2rVjJbj6FeesH92mCYddyjLyL7WyapfyunrPMKn3ty8VZ6zv+9oFq9+QPalHI9tyao7YJ9m1L2uZ5ox6g6B9CGe0FCe4EUW1oXjh6UbxtgsUK7qGlj6UtsW0Spfw+ec5hX+DuPOg5RftTrPFcynhtqGjPu3/ah+ozBMVy757bTurvVsQ4p8yLT/s5kLHOeDXquZKsF963HtG409TzoBetuAynWOlvktJebkltrfKgBNNfK5kr7q8VfREREREREZMIsnMW/lyYD1pKqr00/t0z0goG0Qap6KfvmWYVb5UXPGtOmFumlx+J4vk8N2ghtmphkXHmmPlnxrAFtplzX1CWrudVa0q4w99JotVabXjq/NshftYCybV6QoKlBatFjjz02SXLmmWcmSe573/sOx6CAoA7qyj/1gBUN60dNHcM2rCS1z1BHbdDMmt6mresa+O/yyy9PsjZL0lSh3/RSh0EvPdlqVpd6fG+cA/rpMvUXYNygnHsB+Hrp/NpybJVMyUolX62zNhgg5/fmhV7KU46b2jzCb8RSPi9gJRbM+ozVlnVPGdYq8mq9tOkVeykyKXPm+aosa1MFLrNFs45dfO4F9WuZpzhq+2dVBbS0qbSXgZ6SlW1Yd+v80D5H94JOt4ERe6qkNuhs7ZM8m/Se1ZdR5Sx9tPiLiIiIiIiITJiFs/hjpWqtyMl8XzMsXqx6VevYssDqYV3lbVd1WWGfl+6wl96qXU2s57cr+z2LAse3x1Z6agTqFYsA+2q6mNaSNyXwFWbVt2eVmlc3bdnUsm2taNRJbTPs22abbW7eD5gAWND5K+sfrMXzxobWWlnpKZva+CL0l6q2YI6aekyMebSW/2Rt88c8y/K89FetFa1nfW5TltX795RmUwALeauUrEoKfjMKpGpBbONd9Ob31qpZ97WxMHr9jLLH2rzDDjusOIbzSNMlsiXoKSjbdGy959g2Fsy82Em9+7XH1/eedqyqfbsXv0mWEy3+IiIiIiIiIhPGF38RERERERGRCbNwUv+rrroqSV92t5Z0fsjY1hL4ZGogH6pycKRBuD60wXSS0TWAMqtSwDYwTy9gUyv3qxJL5J5t2p8qZ6rfJZlNR8d34ncQVKW6ckw5qAmBmXop3ZCgUV+UQ60P6gvZZ5WtrRaEphfQTBmZLBIE16tte7U0VvNcm+r5jGX0BfpWT7Ze05ItC4zbjNV1Hmrl/L2UrZRjT9bfBuWr+6iHeanl2oCZtc56c+IUaVPqJuP8ihtT7Qttqj+ox1D2/K3PXbSD9ph6f1w7r7766iTJzjvvPOyjPuh3U68fWV+06QqTlVL/Oj+0QZbXOq9Am+q093xGH+gFBZ7yc7BsHFr8RURERERERCbMwln8r7zyyiTjylZdHSbQWQ9Wl1l1m5eaZKqwar/HHnus2Ef59FbPWdFntb5ad9t0etRHL+Ubx9ayJw1MmxawrmK2qoQaSI5rckwvENOUAzliqafcqhqDffOCyHA8f3uB+1prXD2Gcsb6U1MqzUsVJHJrQnvtWTDnpVRqlU3V2rOaRaXXp3rpSqdOm7ayNy63Ad+S2fJP+vXSplLsBQ5sFW89yz/zSZ3/mPdQk00FAn+1lv7aJ6iryy67LMlsANM2LWmPdl6pgS45j+tQHzXVJQrPK664Ikk/oCNMLfiirG9obz2FC+8pFVJitoH/euNYz+LfpkHlf1L4Jcm1116bZFTKzOsvsrxo8RcRERERERGZMAtn8WcFGGtiL1VSD/wLWcn/5je/ubm+4rrn/PPPHz7vueeeSUYrfOvPn4wrm6ws1hVKVu1X89dLxpVGrll99LG+tD7iPYs/9VsVA1wTywT1W+/fS3E3FbCO9FKPUYZtqsMe8/a1lv9atrQXVqrvfve7D/suvvjitf0IkS0MbbpadhnX2r7Q6xuMab1xhr7QU9roZznOI704Mr0ya/33e76x0FM+tddp1WX1u2BFq88VzC1TnUfaubMq6lA5XHDBBUmSE088cYt+t7ve9a5JkkMPPTTJrJKjjTHA/FRjPdVnDZFNSc/HnnHj0ksvTTJa4Cttasyexb9NsZmsVNcy92DdT5JLLrkkyaguuMtd7jLs68XfkuVEi7+IiIiIiIjIhFk4iz+ru6y2Vb+VO93pTquex+owq3M9H5ploa4Q4rOHpZYV9m233XY4BmsMfkvVf4kVeFYhObb6d1NXvejIbMMKQ3326odV0LqKybWI+ouFghXPZNZ6M3VqNGz6Q+u3Wn0yWQXu+fi3EfvpQ1Ut00banpdZQ2S9QD+p4wzqGdpyGz8jWemXWS3TjIWtv3n1wWQsqrEwlo2ej387jlT1HmXF2NQqKup5UMcx6qWts14UeL5TtSyjKqtzyhSgffKbmQNqrCQsllgwtzTcH5VfzYbBc0CrAKl1J7K5YBypMSnauBef//znh32nn356kpXjTm2/rcW/zj08U89TZ9Zn42T2najN3KQCYHnR4i8iIiIiIiIyYXzxFxEREREREZkwC6d3/9KXvpQkOfbYY5PMyro+97nPrXreMccck2QMrHHcccdtrq+47qkSy/POOy/JKDEiGM4555wzHMO2XooSZEfInlrpf92HbKnWGbInJLc9GRLHI2MitU8yykYJJEdwk953nSIf/OAHk4xSyB133HHYt9122yUZgzVR/lVGi6SM8q711qaPQfJayx8pJuX+1a9+9Zb/KJHNzL/9278lmQ10ere73S3JKN1EGlndV+g7PTkx/YN9jJvXXHPNcAyuSFUCumycddZZSZIPf/jDw7avf/3rScbxv7os8blN+9abI3Cl6LmTcXwrca/HMA/W86krAtxNBVxQTjjhhCRju995552HY5Anb9iwYQt/u1lOOumkJMm//uu/DtuQPn/kIx9JkpxxxhlJlsu1T249vvzlLyeZfZfgGbU3VvRci5JZ6T6fea7dWJhfPvShDyVJdtlll2HfF7/4xRX3k+VEi7+IiIiIiIjIhNmqBo8QERERERERkWmhxV9ERERERERkwvjiLyIiIiIiIjJhfPEXERERERERmTC++IuIiIiIiIhMGF/8RURERERERCaML/4iIiIiIiIiE8YXfxEREREREZEJ44u/iIiIiIiIyITxxV9ERERERERkwvjiLyIiIiIiIjJh/j8/5omjhYx5PAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1296x288 with 30 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(18, 4))\n",
    "originalShape = trainData[\"DataSize\"][0]\n",
    "denseShape = (8, 8)\n",
    "\n",
    "for i, image_idx in enumerate(randomTestImg):\n",
    "    # plot original image\n",
    "    ax = plt.subplot(3, num_images, i + 1)\n",
    "    plt.imshow(X_testE[image_idx].reshape(originalShape))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    # plot encoded image\n",
    "    ax = plt.subplot(3, num_images, num_images + i + 1)\n",
    "#     plt.imshow(encoded_imgs[image_idx].reshape(denseShape))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # plot reconstructed image\n",
    "    ax = plt.subplot(3, num_images, 2*num_images + i + 1)\n",
    "    plt.imshow(encoderPrediction[image_idx].reshape(originalShape))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informe de los resultados para las pruebas del MLP\n",
    "print(\"----- Reporte de Pruebas AUTOENCODER con div aleatoria -----\")\n",
    "# print(\"--- Conteo ---\\n\" + str(Counter(splitPrediction)))\n",
    "# print(\"--- Matriz de Confusion ---\\n\" + str(confusion_matrix(y_testB, splitPrediction)))\n",
    "# print(\"--- Reporte de Pruebas: ---\")\n",
    "# print(classification_report(y_testB, splitPrediction))\n",
    "print(\"--- Puntaje ---\\n\")\n",
    "print(\" - Perdida: \", evalEnconder[0])\n",
    "print(\" - Precision: \", evalEnconder[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curva de aprendizaje para clasificador con sklearn split\n",
    "pd.DataFrame(historyEncoder.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PERCEPTRON MULTICAPA MODIFICADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se remueve la clase a predecir \n",
    "dropColumns = [\"ID\", \"Label\", \"Class\", \"DataSize\", \"ReshapeSize\"]\n",
    "#[\"Name\", \"Path\", \"Digit\", \"DataSize\", \"ReshapeSize\"]\n",
    "X = trainData.drop(columns = dropColumns, axis = 1)[\"Data\"].tolist()\n",
    "y = trainData[\"Label\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agrego las capas necesarias para el modelo Keras Perceptron Multicapa (MLP)\n",
    "# parametros para las capas\n",
    "neurons = 300\n",
    "categories = len(classNamesList)\n",
    "act = \"relu\"\n",
    "out = \"softmax\"\n",
    "ldrop = 0.2\n",
    "\n",
    "# estos se cambian aparte porque se demoran mucho\n",
    "# inshape = list(trainData[\"DataSize\"].value_counts().index.tolist()[0],)\n",
    "# lshape = list(trainData[\"ReshapeSize\"].value_counts().index.tolist()[0],)\n",
    "inshape = list(trainData[\"DataSize\"][0],)\n",
    "lshape = list(trainData[\"ReshapeSize\"][0],)\n",
    "\n",
    "# parametros para compilar\n",
    "l = \"categorical_crossentropy\"\n",
    "opti = \"adam\"\n",
    "met = [\"accuracy\"]\n",
    "\n",
    "# parametros para mirar el progreso del entrenamiento\n",
    "ver = 1\n",
    "epo = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición de las capas MLP\n",
    "Para el MLP se definen las siguientes capas:\n",
    "\n",
    "-\tUna capa de entrada “Flatten” que recibe la imagen 2D y lo transforma a un arreglo 1D para facilidad de procesamiento.\n",
    "-\tUna capa de 300 neuronas “Dense” como capa de entrada para los datos con representación 1D.\n",
    "-\tUna capa “Dropout” con 20% de desactivación de los datos de entrada para evitar un sobre ajuste del modelo, se recalca que entre cada capa de trabajo se utiliza “Dropout” con el mismo propósito.\n",
    "-\tUna capa de 200 neuronas “Dense” como capa como primera capa de aprendizaje.\n",
    "-\tUna capa de 100 neuronas “Dense” como capa como segunda capa de aprendizaje.\n",
    "-\tTodas las capas de aprendizaje tienen función de activación “relu” para evitar la explosión/desvanecimiento de los gradientes.\n",
    "-\tUna capa con 10 neuronas “Dense” como capa de salida para clasificación con función de activación “softmax” para reconocer categorías por separado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arreglo de capas Keras para el MLP\n",
    "layerList = (\n",
    "    Flatten(input_shape = inshape),\n",
    "    Dense(neurons, input_shape = inshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "    Dense(neurons-100, input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "    Dense(neurons-200, input_shape = lshape, activation = act),\n",
    "    Dropout(ldrop, seed = 42),\n",
    "    Dense(categories, input_shape = lshape, activation = out)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entrenamiento Preliminar\n",
    "Al momento de entrenar el modelo preliminar se analizan los comportamientos de dos formas de distribución de datos de entrenamiento. El que tiene por defecto la fuente de datos y uno aleatorio creado para el taller.\n",
    "\n",
    "Cabe recordar que para poder entrenar el modelo multiclase se deben hacer ajustes de tipo (casts) en las entradas de los datos, en donde se transforma la matriz de datos de entrada en un array NumPy (ej.: “np.array(X_testB)”) y se transforma a categorías Dummy la salida (ej.: “to_categorical(np.array(y_testB), categories)”).\n",
    "\n",
    "Siempre se utiliza la división aleatoria de datos después de los experimentos preliminares de la primera parte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Con Split Random SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# division de poblacion de entrenamiento y pruebas con los datos en un orden alternativos\n",
    "X_trainB, X_testB, y_trainB, y_testB = train_test_split(X, y, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definiendo el modelo MLP de Keras\n",
    "splitClassifier = Sequential(layerList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compilando las condiciones de optimizacion y ajuste del MLP Keras\n",
    "splitClassifier.compile(loss = l, optimizer = opti, metrics = met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajustando el modelo MLP Keras\n",
    "historySplit = splitClassifier.fit(\n",
    "    x = np.array(X_trainB), \n",
    "    y = to_categorical(np.array(y_trainB), categories), \n",
    "    epochs = epo, verbose = ver, \n",
    "    validation_data = (np.array(X_testB), to_categorical(np.array(y_testB), categories))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluando el modelo entrenado con sus propias herramientas\n",
    "evalSplit = splitClassifier.evaluate(x = np.array(X_testB), y = to_categorical(np.array(y_testB), categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Perdida: \", evalSplit[0])\n",
    "print(\"Precision: \", evalSplit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumen de datos del MLP Keras\n",
    "splitClassifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se prueba MLP de sklearn\n",
    "splitPrediction = splitClassifier.predict(np.array(X_testB))\n",
    "\n",
    "# ajuste de las predicciones para ver el reporte de matrix de confusion\n",
    "splitPrediction = np.array(splitPrediction).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informe de los resultados para las pruebas del MLP\n",
    "print(\"----- Reporte de Pruebas MLP con div aleatoria -----\")\n",
    "print(\"--- Conteo ---\\n\" + str(Counter(splitPrediction)))\n",
    "print(\"--- Matriz de Confusion ---\\n\" + str(confusion_matrix(y_testB, splitPrediction)))\n",
    "print(\"--- Reporte de Pruebas: ---\")\n",
    "print(classification_report(y_testB, splitPrediction))\n",
    "print(\"--- Puntaje ---\\n\")\n",
    "print(\" - Perdida: \", evalSplit[0])\n",
    "print(\" - Precision: \", evalSplit[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curva de aprendizaje para clasificador con sklearn split\n",
    "pd.DataFrame(historySplit.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimización de Hiper-parámetros\n",
    "\n",
    "para optimizar el desempeño del Perceptrón se utiliza la librería Talos porque es relativamente fácil de crear un modelo optimizable e iterable en el espacio de hiper-parámetros definiendo una función con parámetros específicos. Para lograr esto se realizan los siguientes pasos:\n",
    "\n",
    " - Se define una función como modelo para el modelo de optimización Talos.\n",
    " - Se define un espacio de búsqueda de hiper-parámetros del MLP.\n",
    " - Se ejecuta una búsqueda aleatoria (opción por defecto de Talos) en una fracción reducida del espacio de hiper-parámetros del MLP.\n",
    " - Después, se analizan los resultados para obtener la mejor versión del modelo entrenado y se resume los resultados del modelo seleccionado durante el proceso de optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefiniendo el modelo preliminar en funcion para optimizar\n",
    "def fashionPerceptron(X_train, y_train, X_test, y_test, params):\n",
    "  \n",
    "    inshape = X_train[0].shape\n",
    "    lshape = np.reshape(X_train[0], -1).shape\n",
    "    ver = 0\n",
    "    \n",
    "    # definiendo el modelo MLP de Keras\n",
    "    model = Sequential()\n",
    "    \n",
    "    # capa para representacion alternativa de (n,m) a (m*m)\n",
    "    model.add(Flatten(input_shape = X_train[0].shape))\n",
    "\n",
    "    # capa de entrada para el modelo\n",
    "    model.add(Dense(params[\"first_neuron\"], input_shape = inshape, activation = params[\"activation\"]))\n",
    "    \n",
    "    # paso intermedio para hacer dropout yevitar overfit\n",
    "    model.add(Dropout(params[\"dropout\"]))\n",
    "    \n",
    "    # capas ocultas del modelo\n",
    "    for i in range(params['hidden_layers']):\n",
    "        model.add(Dense(params['hidden_neuron'], input_shape = lshape, activation = params[\"activation\"])),\n",
    "        model.add(Dropout(params['dropout']))\n",
    "    \n",
    "    # capa de salida del modelo\n",
    "    model.add(Dense(params[\"last_neuron\"], input_shape = lshape, activation = params[\"last_activation\"]))\n",
    "    \n",
    "    # compilando las condiciones de optimizacion y ajuste del MLP Keras\n",
    "    model.compile(optimizer = params[\"optimizer\"], loss = params[\"losses\"], metrics = [\"acc\"])\n",
    "\n",
    "    # entrenando el modelo MLP Keras\n",
    "    history = model.fit(\n",
    "        x = np.array(X_train), \n",
    "        y = to_categorical(np.array(y_train), params[\"last_neuron\"]), \n",
    "        epochs = params[\"epochs\"], verbose = ver, \n",
    "        workers = 8, use_multiprocessing = True,\n",
    "        batch_size = params[\"batch_size\"],\n",
    "        callbacks = [talos.utils.ExperimentLogCallback(\"fashionPerceptronExp\", params)],\n",
    "        validation_data = (np.array(X_test), to_categorical(np.array(y_test), params[\"last_neuron\"]))\n",
    "    )\n",
    "    \n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# espacio de optimizacion de hyperparametros para el MLP\n",
    "parameters = {\"activation\": [\"relu\", \"elu\"],\n",
    "     \"last_activation\": [\"softmax\"],\n",
    "     \"optimizer\": [\"Nadam\", \"Adam\"],\n",
    "     \"losses\": [\"categorical_crossentropy\"],\n",
    "     \"shapes\": [\"brick\"],\n",
    "     \"first_neuron\": [100, 200, 300],\n",
    "     \"hidden_neuron\": [100, 200, 300],\n",
    "     \"last_neuron\": [len(classNamesList)],\n",
    "     \"hidden_layers\": [0, 1, 2, 3],\n",
    "     \"dropout\": [.1, .2, .3],\n",
    "     \"batch_size\": [32, 64, 128],\n",
    "     \"epochs\": [10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# se hace la busqueda con la funcion Scan()\n",
    "# se utiliza el conjunto de datos reorganizado con sklearn\n",
    "# la semilla random es 42 porque es la respuesta universal y es la misma en todo el codigo\n",
    "# por defecto Talos hace una busqueda random, en general es mejor aunque puede caer en minimos/maximos locales\n",
    "# la poblacion de busqueda aleatoria es de un 1.0% ~ 3.0% para que el procedimiento sea rapido\n",
    "# para acelerar el proceso se prueba con reduction_method con correlacion y ajustandolo contra la precision de validacion\n",
    "scanPerceptron = talos.Scan(x = X_trainB, y = y_trainB,\n",
    "                            x_val = X_testB, y_val = y_testB,\n",
    "                            model = fashionPerceptron,\n",
    "                            experiment_name = \"fashionPerceptronExp\",\n",
    "                            params = parameters, \n",
    "                            reduction_method = \"correlation\",\n",
    "                            reduction_metric = \"val_acc\",\n",
    "                            fraction_limit = 0.03,\n",
    "                            seed = 42,\n",
    "                            print_params = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accediendo a los resultados del proceso de Optimización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mirando los resultados de la optimizacion con dataFrame\n",
    "scanPerceptron.data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mirando la entropia de cada ronda de experimentos\n",
    "scanPerceptron.learning_entropy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tipos de datos dentro del set de resultados\n",
    "scanPerceptron.data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ver los modelos que se probaron en el proceso de optimizavion\n",
    "# scanPerceptron.saved_models\n",
    "    \n",
    "# # ver los pesos de los modelos probados en el proceso de optimizacion\n",
    "# scanPerceptron.saved_weights\n",
    "\n",
    "# resumen del proceso de optimizacion realizado\n",
    "scanPerceptron.details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analizando los resultados del proceso de Optimización\n",
    "Se hacen diferentes graficas para tener perspectivas diferentes en el análisis de los resultados de la búsqueda con Talos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para el analisis se utiliza el reporte creado por Talos como entrada de Analyze\n",
    "analyzePerceptron = talos.Analyze(scanPerceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acceso al DataFrame del analisis del reporte\n",
    "analyzePerceptron.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numero de experimentos realizados\n",
    "analyzePerceptron.rounds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se recupera el numero del modelo con mejor puntaje, en este caso es la precision de pruetas/validacion \"val_acc\"\n",
    "analyzePerceptron.high(\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the round with the best result\n",
    "analyzePerceptron.rounds2high(\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imprime los mejores parametros encontrados en el proceso de optimizacion\n",
    "analyzePerceptron.best_params(\"val_acc\", [\"acc\", \"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlacion entre el puntaje y el resto de los hyperparametros\n",
    "analyzePerceptron.correlate(\"val_loss\", [\"acc\", \"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a regression entre precision de puntaje/precision y perdida del conjunto de preubas\n",
    "analyzePerceptron.plot_regs(\"val_acc\", \"val_loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linea de resultados de la precision de los experimentos\n",
    "analyzePerceptron.plot_line(\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distribucion de probabilidad de la precision de pruebas\n",
    "analyzePerceptron.plot_kde(\"val_acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograma de la precision de pruebas de los experimentso\n",
    "analyzePerceptron.plot_hist(\"val_acc\", bins = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapa de correlacion para la correlacion de de la precision de preubas y otras variables\n",
    "analyzePerceptron.plot_corr(\"val_loss\", [\"acc\", \"loss\", \"val_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica analizando el comportamiento de las capas ocultas, precision de pruebas, # de neuroans en la entrada y # de neuronas en la capa escondida\n",
    "analyzePerceptron.plot_bars(\"hidden_layers\", \"val_acc\", \"first_neuron\", \"hidden_neuron\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica analizando el comportamiento de las capas ocultas, precision de pruebas, # de neuroans en la entrada y # de neuronas en la capa escondida\n",
    "analyzePerceptron.plot_bars(\"hidden_layers\", \"val_loss\", \"first_neuron\", \"hidden_neuron\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluando finalmente el Modelo\n",
    "por defecto Talos si no se indica el ID del modelo para evaluar, escoge el mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creando el objeto para evaluar con Talos\n",
    "evaluatePerceptron = talos.Evaluate(scanPerceptron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluacion final de los datos con el mejor modelo\n",
    "bestAcc = evaluatePerceptron.evaluate(np.array(X_testB), to_categorical(np.array(y_testB)), folds = 5, metric = \"val_acc\", task = \"multi_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resumen basico del mejor MLP\n",
    "evaluatePerceptron.scan_object.best_model(metric='val_acc').summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuracion especifica del mejor MLP\n",
    "evaluatePerceptron.scan_object.best_model(metric='val_acc').get_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicciones y Análisis detallado del mejor Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predecir resultados con el mejor modelo\n",
    "predictPerceptron = talos.Predict(scanPerceptron, task = \"multi_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se prueba MLP de sklearn\n",
    "bestPrediction = predictPerceptron.predict(np.array(X_testB))\n",
    "\n",
    "# ajuste de las predicciones para ver el reporte de matrix de confusion\n",
    "bestPrediction = np.array(bestPrediction).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informe de los resultados para las pruebas del MLP\n",
    "print(\"----- Reporte FINAL de Pruebas MLP -----\")\n",
    "print(\"--- Conteo ---\\n\" + str(Counter(bestPrediction)))\n",
    "print(\"--- Matriz de Confusion ---\\n\" + str(confusion_matrix(y_testB, bestPrediction)))\n",
    "print(\"--- Reporte de Pruebas: ---\")\n",
    "print(classification_report(y_testB, bestPrediction))\n",
    "print(\"--- Puntaje ---\\n\")\n",
    "print(\" - Precision: \", str(bestAcc))\n",
    "print(\" - Precision Promedio: \", str(np.mean(bestAcc)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones \n",
    "\n",
    "Antes de entrenar cuando se cargan los datos, aunque no es absolutamente necesario, se crea un DataFrame de Pandas para facilitar la lectura y el reusó. Se guarda la forma y otros valores de posible interés en el momento de entrenamiento. Al utilizar la división aleatoria para entrenar el modelo preliminar se ve un contraste marcado en la curva de aprendizaje, en donde se observa un comportamiento convergente tradicional entre la precisión/perdida en pruebas y entrenamiento. Por su parte, el comportamiento del conjunto de datos organizados por defecto muestra un comportamiento casi paralelo entre y estable de la precisión/perdida lo cual indica un modelo sub ajustado. Esto último valida la decisión de utilizar una división aleatoria del conjunto de datos y la creación de un DataFrame aunque su rendimiento/perdida nominal no sean significativamente diferentes.\n",
    "\n",
    "En el entrenamiento preliminar se observa que ambos modelos tienen un comportamiento global general, pero en las clases se observa que las categorías 0, 2, 4 y 6 tienen menos precisión y en general se confunden entre ellas. Al ver los datos y los nombres de las clases se reconoce que estas clases son “T-shirt/top”/camiseta, “Pullover”/buso, “Coat”/chaqueta, “Shirt”/camisa respectivamente. Esto en si da a ver que las clases son realmente similares y parecidas entre si y que una mala distribución de datos afecta el rendimiento del modelo al momento de reconocer las clases. \n",
    "\n",
    "Después de este entrenamiento preliminar siempre se escogerá una distribución aleatoria para la optimización y los otros puntos del taller como distribución por defecto para los datos.\n",
    "\n",
    "En la optimización de hiper-parámetros se utiliza la librería por extensión Talos por su facilidad al transformar las especificaciones del modelo preliminar en modelos candidatos durante una búsqueda aleatoria de los mejores hiper-parámetros. La búsqueda se hace aleatoria, con la precisión como parámetro objetivo a maximizar y con una población de máximo 3% para no cargar de tantos datos el proceso. En general se configura la búsqueda para minimizar el uso de parámetros/hiper-parámetros no significativos (correlacionados linealmente) con la precisión del modelo.\n",
    "\n",
    "El modelo final optimizado es un MLP con:\n",
    "\n",
    "-\tUna capa de entrada “Flatten” que recibe la imagen 2D y lo transforma a un arreglo 1D para facilidad de procesamiento.\n",
    "-\tUna capa de 300 neuronas “Dense” como capa de entrada para los datos y activación tipo “elu”\n",
    "-\tEntre capas de trabajo “Dropout” con 10% de desactivación para evitar sobreajustes.\n",
    "-\tUna capa de 200 neuronas “Dense” como capa como primera capa de aprendizaje y activación tipo “elu”.\n",
    "-\tUna capa de 200 neuronas “Dense” como capa como segunda capa de aprendizaje y activación tipo “elu”.\n",
    "-\tUna capa con 10 neuronas “Dense” como capa de salida para clasificación con función de activación “softmax” para reconocer categorías por separado.\n",
    "\n",
    "La búsqueda muestra una correlación fuerte entre la precisión y la perdida de tendencia lineal, lo que coincide con la literatura. La precisión tiene un mínimo de 35% y un máximo de 86% (optimo encontrado). En promedio el rendimiento (precisión) de los modelos esta alrededor 78%. También se observa una correlación entre el tamaño de la muestra lo cual muestra que es una opción viable para agilizar el entrenamiento. Otra correlación esta en los valores de número de capas ocultas y neuronas dentro de ellas, lo cual soporta la idea que se necesitan mas capas de aprendizaje para distinguir las diferencias entre las clases confusas (0, 2, 4 y 6). Por último, el Dropout también se observa la correlación al evitar el sobreajuste.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
