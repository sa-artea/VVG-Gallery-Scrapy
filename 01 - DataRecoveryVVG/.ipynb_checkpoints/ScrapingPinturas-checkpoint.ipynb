{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraer datos de WEB\n",
    "\n",
    "Este Script se concentra en extraer la informacion y datos de la galeria del museo Van Gogh y las cartas escritas por el mismo artista para guardar los archivos de texto, png y otros datos necesarios para crear el set de datos de entrenamiento para un modelo de Machine Larning. Se utiliza BeautifulSoup para extraer la informacion desde los portales WEB de interes\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Extraer las imagenes, datos y metadatos de cada elemento dentro de la coleccion del museoVan Gogh.\n",
    "\n",
    "## Fuente de Datos\n",
    "\n",
    "Las fuentes de datos para el conjunto de datos son:\n",
    "\n",
    "* Galeria de obras y trabajo de Van Gogh: https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh\n",
    "* Archivo de cartas escritas por Van Gogh: http://vangoghletters.org/vg/search/simple?term=\n",
    "* Ruta de carpetas de recursos de las cartas escritas por Van Gogh: http://vangoghletters.org/vg/letters/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias necesarias\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import os\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# from os import remove\n",
    "# import os.path\n",
    "# from os import remove\n",
    "# from numpy import array\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import requests, pandas, numpy, matplotlib.pyplot\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# import urllib\n",
    "# import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagina Fuente (Root Page)\n",
    "\n",
    "desde la pagina fuente recupero todos los enlaces a las obras de Vangohg y despues con la lista de enlaces recupero la informacion necesaria\n",
    "\n",
    "Enlaces de interes:\n",
    "- Extract links from webpage (BeautifulSoup): https://pythonspot.com/extract-links-from-webpage-beautifulsoup/\n",
    "- How to: Find all tags with some given name and attributes: https://kite.com/python/examples/1734/beautifulsoup-find-all-tags-with-some-given-name-and-attributes\n",
    "- Beautiful Soup can't find the part of the HTML I want: https://stackoverflow.com/questions/51982930/beautiful-soup-cant-find-the-part-of-the-html-i-want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galeria de Obras de vangoghmuseum.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Search URL ---\n",
      "https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=30\n"
     ]
    }
   ],
   "source": [
    "# Se recorre la pagina principal y recuperan los enlaces de l \n",
    "# numero de paginas/obras/enlaces a iterar\n",
    "\n",
    "paints = 100\n",
    "defaultPaints = 50\n",
    "\n",
    "# maximo numero de pinturas conocidas\n",
    "maxPaints = 1443\n",
    "\n",
    "# busqueda base en la coleccion de Vangogh\n",
    "paintsSearch = \"https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=\"\n",
    "paintsRoot = \"https://vangoghmuseum.nl\"\n",
    "\n",
    "# por defecto pruebo 50 elementos de la coleccion\n",
    "paintsPage = paintsSearch + str(defaultPaints)\n",
    "\n",
    "# si el numero de enlaces abuscar es menor al maximo de la coleccion conocida\n",
    "if paints <= maxPaints:\n",
    "    paintsPage = paintsSearch + str(paints)\n",
    "\n",
    "# si hay algo raro\n",
    "else:\n",
    "    paintsPage = paintsSearch + str(defaultPaints)\n",
    "\n",
    "# reviso que cargue bien la pagina base\n",
    "print(\"--- Search URL ---\")\n",
    "print(paintsPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pido el HTML\n",
    "paintsPage = urllib.request.urlopen(paintsPage)\n",
    "\n",
    "# uso beautifulSoup\n",
    "soup = BeautifulSoup(paintsPage, \"html.parser\")\n",
    "\n",
    "# lista de enlaces a elementos de la coleccion\n",
    "links = list()\n",
    "\n",
    "# esta es la seccion del HTML donde estan los resultados de la coleccion NO LA NECESITO POR AHORA\n",
    "# collectionSoup = soup.body.findAll(\"ul\", attrs = {\"class\":\"cols cols-3up list-plain\"})\n",
    "linkSoup = soup.body.findAll('a', attrs = {\"class\":\"link-teaser triggers-slideshow-effect\", \"href\":re.compile(\"^/en/collection/\")})\n",
    "\n",
    "# saco los links a las paginas que quiero dentro de la busqueda de coleccion\n",
    "# estan los elementos de tipo link-teaser y la expresion regular del href\n",
    "for link in linkSoup:\n",
    "    \n",
    "    # reconstruyo el enlace completo\n",
    "    # tempLink = paintsRoot + link.get('href')\n",
    "    tempLink = urllib.parse.urljoin(paintsRoot, link.get('href'))\n",
    "    links.append(tempLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links in search: 30\n"
     ]
    }
   ],
   "source": [
    "#chequeo los links de la busqueda\n",
    "print(\"Extracted links in search: \" + str(len(links)))\n",
    "for link in links:\n",
    "    pass\n",
    "#     print(link)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directorio y Carpetas para persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\OneDrive - Universidad de Los Andes\\03 - PhD\\04 - Clases\\05 - IA en Arte y Disenho\\03 - Proyecto\\01 - Data\n"
     ]
    }
   ],
   "source": [
    "# defino los directorios locales donde se va a persistir la informacion\n",
    "# no olvidarse del cambiar \"\\\" a \"\\\\\" en los filepath de windows por que si no no sirve nada\n",
    "# dir de santiago\n",
    "SFAM_ROOT = \"C:\\\\Users\\\\Felipe\\\\OneDrive - Universidad de Los Andes\\\\03 - PhD\\\\04 - Clases\\\\05 - IA en Arte y Disenho\\\\03 - Proyecto\\\\01 - Data\\\\01 - Raw\"\n",
    "# dir de daniela\n",
    "DCP_ROOT = \"\"\n",
    "# variable intermedia para independizar, se comenta uno u otro segun donde se corra\n",
    "WORK_ROOT = SFAM_ROOT\n",
    "# WORK_ROOT = DCP_ROOT\n",
    "\n",
    "# nombres de carpetas utiles donde se guarda la informacion\n",
    "rawFolder = \"01 - Raw\"\n",
    "paintsFolder = \"01 - Paints\"\n",
    "lettersFolder = \"02 - Letters\"\n",
    "\n",
    "#La ruta se obtiene con el dirpath y el filename y el dirpath.split(os.path.sep)[-1] agrega la clase\n",
    "DATA_ROOT = os.path.dirname(WORK_ROOT)\n",
    "print(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion de funciones para extraer Informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de las pinturas en la galeria\n",
    "\n",
    "el proceso sigue los siguientes pasos, para facilidad de la estructura se propone un formato JSON por cada uno de los frames que se desean guardar en formato TXT, al final cada carpeta debe tener 4 archivos TXT mas una image en formato PNG. Los pasos a seguir son los siguientes:\n",
    "\n",
    "* Chequeo que los enlaces existan y creo las carpetas necesarias.\n",
    "* Extraigo las anotaciones de busqueda cada uno de los elementos de la galeria desde el objeto \"Search in the collection:\".\n",
    "* Extraigo los datos archivisticos de cada elemento de la galeria desde el objeto \"OBJECT DATA\".\n",
    "* Extraigo los trabajos relacionados si existen de cada elemento de la galeria desde el objeto \"Related work\".\n",
    "* Extraigo la imagen en formato PNG de cada elemento de la galeria en el desde el objeto \"DOWNLOAD IMAGE\"\n",
    "\n",
    "\n",
    " #### Enlaces utiles para el proceso\n",
    " \n",
    "- Como chequear que un enlace esta vivo: https://stackoverflow.com/questions/51639585/checking-if-a-website-exist-with-python3\n",
    "- Como utilizar libreria Request y el metodo GET: https://realpython.com/python-requests/\n",
    "- Manejo de errores en Python 3 URL 1: https://www.python-course.eu/python3_exception_handling.php\n",
    "- Manejo de errores en Python 3 URL 2: https://www.tutorialspoint.com/python3/python_exceptions.htm\n",
    "- descargar una imagen desde un URL: https://stackabuse.com/download-files-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explored collection links: 30\n",
      "New folders created: 0\n",
      "Downloaded images: 0\n",
      "Repeated images: 30\n"
     ]
    }
   ],
   "source": [
    "# recorro el arreglo de enlaces habilitados para extraer la informacion\n",
    "# variables de conteo del proceso\n",
    "folderCount = 0\n",
    "downloadCount = 0\n",
    "linkCount = 0\n",
    "repeatCount = 0\n",
    "\n",
    "#inicio del ciclo para los enlaces\n",
    "for link in links:\n",
    "    \n",
    "    # recojo la informacion necesaria para crear las carpetas y guarar los datos\n",
    "    linkPar = urlparse(link)\n",
    "    linkLen = len(linkPar.path.split(\"/\"))\n",
    "    linkFolder = linkPar.path.split(\"/\")[linkLen-1]\n",
    "        \n",
    "    # chequeo si el enlace sirve para sacar la informacion\n",
    "    try:\n",
    "        # GET del URL\n",
    "        linkReq = requests.get(link)\n",
    "        \n",
    "        # si el GET me responde bien con codigo 200\n",
    "        if linkReq.status_code == 200:\n",
    "            \n",
    "            linkCount = linkCount + 1\n",
    "            \n",
    "            # si la ruta de trabajo local esta definida\n",
    "            if os.path.exists(WORK_ROOT):\n",
    "                \n",
    "                # concateno los folders para la nueva carpeta de trabajo\n",
    "                workPath = os.path.join(DATA_ROOT, rawFolder, paintsFolder)\n",
    "                \n",
    "                # si todo va bien en el nuevo folder de trabajo\n",
    "                if os.path.exists(workPath):\n",
    "                    \n",
    "                    # creo path para cada elemento en la coleccion\n",
    "                    tempPaintFolder = os.path.join(workPath, linkFolder)\n",
    "                    \n",
    "                    # creo la carpeta del nuevo elemento de la coleccion si no existe\n",
    "                    if not os.path.exists(tempPaintFolder):\n",
    "                        \n",
    "                        os.makedirs(tempPaintFolder)\n",
    "                        # cuento cuantos folderes nuevos creo\n",
    "                        folderCount = folderCount + 1\n",
    "                    \n",
    "                    # si existe la carpeta, descargo la imagen de la obra dentro de la coleccion de una vez\n",
    "                    if os.path.exists(tempPaintFolder):\n",
    "                        \n",
    "                        # parse del cuerpo del elemento de la coleccion\n",
    "                        linkSoup = BeautifulSoup(linkReq.content, \"html.parser\")\n",
    "                        \n",
    "                        # busco todos los elementos de tipo class=\"button dark-hover\"\n",
    "                        soupDict = {\"class\":\"button dark-hover\", \"href\":re.compile(\"^/download/\")}\n",
    "                        downloadSoup = linkSoup.find('a', attrs = soupDict)\n",
    "#                         print(downloadSoup)\n",
    "                        \n",
    "                        # creo el enlace para descargar la imagen\n",
    "                        tempLink = urllib.parse.urljoin(paintsRoot, downloadSoup.get(\"href\"))\n",
    "#                         print(tempLink)\n",
    "                        \n",
    "                        # pido el enlace de la imagen\n",
    "                        downReq = requests.get(tempLink)\n",
    "                        \n",
    "                        # creo el nombre y la direccion del archivo que quiero guardar en la carpeta\n",
    "                        fileName = urlparse(tempLink)\n",
    "                        fileName = fileName.path.split(\"/\")[len(fileName.path.split(\"/\"))-1]\n",
    "                        filePath = os.path.join(tempPaintFolder, fileName)\n",
    "                        \n",
    "                        # si el archivo no existe lo guardo\n",
    "                        if not os.path.exists(filePath):\n",
    "                            \n",
    "                            print(\"Downloding img from: \" + str(filePath))\n",
    "                            \n",
    "                            with open(filePath, \"wb\") as file:\n",
    "                                file.write(downReq.content)\n",
    "                                file.close()\n",
    "                                downloadCount = downloadCount + 1\n",
    "                                \n",
    "                        elif os.path.exists(filePath):\n",
    "                            repeatCount = repeatCount + 1\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error in request: \" + str(e))\n",
    "        print(\"Status Code: \" + str(linkReq.status_code))\n",
    "\n",
    "print(\"Explored collection links: \" + str(linkCount))\n",
    "print(\"New folders created: \" + str(folderCount))\n",
    "print(\"Downloaded images: \" + str(downloadCount))\n",
    "print(\"Repeated images: \" + str(repeatCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivo de Cartas de vangoghletters.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "totalpinturas=1443\n",
    "temporal=199\n",
    "pagina='https://vangoghmuseum.nl/en/collection/s'\n",
    "a=np.arange(temporal)\n",
    "lista=[]\n",
    "for i in a:\n",
    "    lista.append(pagina+str((i+1)).zfill(4)+'V1962')\n",
    "    URL = lista[i]\n",
    "\n",
    "\n",
    "    def iterate_over_children(soup, texts):\n",
    "        for child in soup:\n",
    "            if child.string == None:\n",
    "                iterate_over_children(child, texts)\n",
    "            else:\n",
    "                if child.string != '\\n':\n",
    "                    texts.append(child.string)\n",
    "\n",
    "    # Realizamos la petición a la web\n",
    "    req = requests.get(URL)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    status_code = req.status_code\n",
    "    if status_code == 200:\n",
    "\n",
    "        # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "        html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "        article = html.find('section')\n",
    "        texts = []\n",
    "        iterate_over_children(article, texts)\n",
    "        texts_cleaned = [] \n",
    "        for text in texts:\n",
    "            cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "            texts_cleaned.append(cleaned_text)\n",
    "\n",
    "    #Daniela Oject Data\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "\n",
    "\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        for i, entrada in enumerate(entradas):\n",
    "            # Con el método \"getText()\" no nos devuelve el HTML\n",
    "            hola= entrada.find('a').get('href') \n",
    "            texts_cleaned.append(hola)\n",
    "            \n",
    "        #links del parrafo (puede ir a las cartas)\n",
    "        link_p=[tag['href'] for tag in html.select('p a[href]')]\n",
    "        #links de search in the collection\n",
    "        link_c=[tag['href'] for tag in html.select('aside a[href]')]\n",
    "            \n",
    "        \n",
    "        #print(texts_cleaned)\n",
    "        os.mkdir(texts_cleaned[0])\n",
    "        #print (texts_cleaned)\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        save_path =texts_cleaned[0]\n",
    "\n",
    "        f=open(texts_cleaned[0]+'.txt', 'w')\n",
    "\n",
    "        completeName = os.path.join(save_path,texts_cleaned[0]+'.txt')    \n",
    "        \n",
    "        f.close()\n",
    "        remove(texts_cleaned[0]+'.txt')\n",
    "\n",
    "        file1 = open(completeName, \"w\")\n",
    "        \n",
    "        tamanio = len(texts_cleaned)\n",
    "        for i in range(0,len(texts_cleaned)):\n",
    "            file1.write(texts_cleaned[i]+'\\n')\n",
    "            \n",
    "        for i in range(0,len(link_p)):\n",
    "            file1.write(link_p[i]+'\\n')\n",
    "        for i in range(0,len(link_c)):\n",
    "            file1.write(link_c[i]+'\\n')\n",
    "        file1.close()\n",
    "      \n",
    "        print('Beginning file download with wget module')\n",
    "\n",
    "        url = 'https://vangoghmuseum.nl'+texts_cleaned[tamanio-1]\n",
    "        wget.download(url, texts_cleaned[0]+'\\ ' +texts_cleaned[0]+'.jpg')\n",
    "\n",
    "        # guardar-paginaweb.py\n",
    "\n",
    "        respuesta = urllib.request.urlopen(URL)\n",
    "        contenidoWeb = respuesta.read()\n",
    "\n",
    "        pag = open(texts_cleaned[0]+'.html', 'wb')\n",
    "        completeName2 = os.path.join(save_path,texts_cleaned[0]+'.html')    \n",
    "        pag.close()\n",
    "        file2 = open(completeName2, \"wb\")\n",
    "        file2.write(contenidoWeb)\n",
    "        file2.close()\n",
    "        \n",
    "        remove(texts_cleaned[0]+'.html')\n",
    "    else:\n",
    "        print (\"Status Code %d\" % status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = html.find('p').getText()\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://vangoghmuseum.nl/download/45214344-2680-447a-92d3-e34667866f59.jpg'\n",
    "wget.download(url, 'Congregation Leaving the Reformed Church in Nuenen\\daniela.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "URL = \"https://vangoghmuseum.nl/en/collection/s0199V1962\"\n",
    "\n",
    "def iterate_over_children(soup, texts):\n",
    "    for child in soup:\n",
    "        if child.string == None:\n",
    "            iterate_over_children(child, texts)\n",
    "        else:\n",
    "            if child.string != '\\n':\n",
    "                texts.append(child.string)\n",
    "\n",
    "# Realizamos la petición a la web\n",
    "req = requests.get(URL)\n",
    "\n",
    "# Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "status_code = req.status_code\n",
    "if status_code == 200:\n",
    "\n",
    "    # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "    html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "    article = html.find('section')\n",
    "    texts = []\n",
    "    iterate_over_children(article, texts)\n",
    "    texts_cleaned = [] \n",
    "    for text in texts:\n",
    "        cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "        texts_cleaned.append(cleaned_text)\n",
    "  #  print(texts_cleaned)\n",
    "       \n",
    "#Daniela Oject Data\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "    \n",
    "\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "    for i, entrada in enumerate(entradas):\n",
    "        # Con el método \"getText()\" no nos devuelve el HTML\n",
    "        hola= entrada.find('a').get('href') \n",
    "        texts_cleaned.append(hola)\n",
    "    print (texts_cleaned)\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "\n",
    "#url_imagen = \"https://golang.org/doc/gopher/appenginegophercolor.jpg\" # El link de la imagen\n",
    "#nombre_local_imagen = \"go.jpg\" # El nombre con el que queremos guardarla\n",
    "#imagen = requests.get(url_imagen).content\n",
    "#with open(nombre_local_imagen, 'wb') as handler:\n",
    "#\thandler.write(imagen)\n",
    "\n",
    "else:\n",
    "    print (\"Status Code %d\" % status_code)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Comedores de Patatas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path ='Comedores de Patatas'\n",
    "\n",
    "f = open('daniela6.txt', 'w')\n",
    "\n",
    "completeName = os.path.join(save_path,'daniela6.txt')         \n",
    "\n",
    "file1 = open(completeName, \"w\")\n",
    "\n",
    "file1.write(\"daniela carcamo\")\n",
    "file1.close()\n",
    "\n",
    "# remove(completeName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abre-paginaweb.py\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "print(contenidoWeb[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar-paginaweb.py\n",
    "\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "f = open('patatas.html', 'wb')\n",
    "f.write(contenidoWeb)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
