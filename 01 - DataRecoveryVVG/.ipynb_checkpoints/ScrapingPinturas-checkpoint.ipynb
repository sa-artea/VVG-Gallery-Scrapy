{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias necesarias\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import os\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# from os import remove\n",
    "# import os.path\n",
    "# from os import remove\n",
    "# from numpy import array\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import requests, pandas, numpy, matplotlib.pyplot\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# import urllib\n",
    "# import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagina Fuente (Root Page)\n",
    "\n",
    "desde la pagina fuente recupero todos los enlaces a las obras de Vangohg y despues con la lista de enlaces recupero la informacion necesaria\n",
    "\n",
    "Enlaces de interes:\n",
    "- Extract links from webpage (BeautifulSoup): https://pythonspot.com/extract-links-from-webpage-beautifulsoup/\n",
    "- How to: Find all tags with some given name and attributes: https://kite.com/python/examples/1734/beautifulsoup-find-all-tags-with-some-given-name-and-attributes\n",
    "- Beautiful Soup can't find the part of the HTML I want: https://stackoverflow.com/questions/51982930/beautiful-soup-cant-find-the-part-of-the-html-i-want\n",
    "\n",
    "EXTRA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=1443\n",
      "1443\n"
     ]
    }
   ],
   "source": [
    "# iterar sobre la pagina principal de coleccion y recoger los links de interes\n",
    "# numero de paginas/obras/enlaces a iterar\n",
    "elements = 1443\n",
    "defaultElements = 50\n",
    "\n",
    "# maximo numero de pinturas conocidas\n",
    "maxElements = 1443\n",
    "\n",
    "# busqueda base en la coleccion de Vangogh\n",
    "root = \"https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=\"\n",
    "home = \"https://vangoghmuseum.nl\"\n",
    "\n",
    "# por defecto pruebo 50 elementos de la coleccion\n",
    "webPage = root + str(defaultElements)\n",
    "\n",
    "# si el numero de enlaces abuscar es menor al maximo de la coleccion conocida\n",
    "if elements <= maxElements:\n",
    "    webPage = root + str(elements)\n",
    "\n",
    "# si hay algo raro\n",
    "else:\n",
    "    webPage = root + str(defaultElements)\n",
    "\n",
    "# reviso que cargue bien la pagina base\n",
    "print(webPage)\n",
    "\n",
    "# pido el HTML\n",
    "rootPage = urllib.request.urlopen(webPage)\n",
    "\n",
    "# uso beautifulSoup\n",
    "soup = BeautifulSoup(rootPage, \"html.parser\")\n",
    "\n",
    "# lista de enlaces a elementos de la coleccion\n",
    "links = list()\n",
    "\n",
    "# esta es la seccion del HTML donde estan los resultados de la coleccion NO LA NECESITO POR AHORA\n",
    "collectionSoup = soup.body.findAll(\"ul\", attrs = {\"class\":\"cols cols-3up list-plain\"})\n",
    "\n",
    "# saco los links a las paginas que quiero dentro de la busqueda de coleccion\n",
    "# estan los elementos de tipo link-teaser y la expresion regular del href\n",
    "for link in soup.body.findAll('a', attrs = {\"class\":\"link-teaser triggers-slideshow-effect\", \"href\":re.compile(\"^/en/collection/\")}):\n",
    "    \n",
    "    # reconstruyo el enlace completo\n",
    "    tempLink = home + link.get('href')\n",
    "    links.append(tempLink)\n",
    "\n",
    "# print(links)\n",
    "print(len(links))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download with wget module\n",
      "100% [............................................................................] 184620 / 184620Beginning file download with wget module\n",
      "100% [............................................................................] 116796 / 116796Beginning file download with wget module\n",
      "100% [............................................................................] 111241 / 111241Status Code 500\n",
      "Beginning file download with wget module\n",
      "100% [............................................................................] 114237 / 114237Beginning file download with wget module\n",
      "100% [..............................................................................] 51245 / 51245Beginning file download with wget module\n",
      "100% [..............................................................................] 67288 / 67288Beginning file download with wget module\n",
      "100% [............................................................................] 133352 / 133352Beginning file download with wget module\n",
      "100% [..............................................................................] 83158 / 83158Beginning file download with wget module\n",
      "100% [..............................................................................] 80622 / 80622Beginning file download with wget module\n",
      "100% [............................................................................] 213215 / 213215Beginning file download with wget module\n",
      "100% [............................................................................] 202815 / 202815Beginning file download with wget module\n",
      "100% [............................................................................] 162602 / 162602Beginning file download with wget module\n",
      "100% [............................................................................] 120058 / 120058"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[WinError 267] El nombre del directorio no es válido: 'Montmartre: Windmills and Allotments'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-13f63eae2140>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;31m#print(texts_cleaned)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexts_cleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m         \u001b[1;31m#print (texts_cleaned)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m         \u001b[1;31m# Recorremos todas las entradas para extraer el título, autor y fecha\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotADirectoryError\u001b[0m: [WinError 267] El nombre del directorio no es válido: 'Montmartre: Windmills and Allotments'"
     ]
    }
   ],
   "source": [
    "\n",
    "totalpinturas=1443\n",
    "temporal=199\n",
    "pagina='https://vangoghmuseum.nl/en/collection/s'\n",
    "a=np.arange(temporal)\n",
    "lista=[]\n",
    "for i in a:\n",
    "    lista.append(pagina+str((i+1)).zfill(4)+'V1962')\n",
    "    URL = lista[i]\n",
    "\n",
    "\n",
    "    def iterate_over_children(soup, texts):\n",
    "        for child in soup:\n",
    "            if child.string == None:\n",
    "                iterate_over_children(child, texts)\n",
    "            else:\n",
    "                if child.string != '\\n':\n",
    "                    texts.append(child.string)\n",
    "\n",
    "    # Realizamos la petición a la web\n",
    "    req = requests.get(URL)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    status_code = req.status_code\n",
    "    if status_code == 200:\n",
    "\n",
    "        # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "        html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "        article = html.find('section')\n",
    "        texts = []\n",
    "        iterate_over_children(article, texts)\n",
    "        texts_cleaned = [] \n",
    "        for text in texts:\n",
    "            cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "            texts_cleaned.append(cleaned_text)\n",
    "\n",
    "    #Daniela Oject Data\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "\n",
    "\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        for i, entrada in enumerate(entradas):\n",
    "            # Con el método \"getText()\" no nos devuelve el HTML\n",
    "            hola= entrada.find('a').get('href') \n",
    "            texts_cleaned.append(hola)\n",
    "            \n",
    "        #links del parrafo (puede ir a las cartas)\n",
    "        link_p=[tag['href'] for tag in html.select('p a[href]')]\n",
    "        #links de search in the collection\n",
    "        link_c=[tag['href'] for tag in html.select('aside a[href]')]\n",
    "            \n",
    "        \n",
    "        #print(texts_cleaned)\n",
    "        os.mkdir(texts_cleaned[0])\n",
    "        #print (texts_cleaned)\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        save_path =texts_cleaned[0]\n",
    "\n",
    "        f=open(texts_cleaned[0]+'.txt', 'w')\n",
    "\n",
    "        completeName = os.path.join(save_path,texts_cleaned[0]+'.txt')    \n",
    "        \n",
    "        f.close()\n",
    "        remove(texts_cleaned[0]+'.txt')\n",
    "\n",
    "        file1 = open(completeName, \"w\")\n",
    "        \n",
    "        tamanio = len(texts_cleaned)\n",
    "        for i in range(0,len(texts_cleaned)):\n",
    "            file1.write(texts_cleaned[i]+'\\n')\n",
    "            \n",
    "        for i in range(0,len(link_p)):\n",
    "            file1.write(link_p[i]+'\\n')\n",
    "        for i in range(0,len(link_c)):\n",
    "            file1.write(link_c[i]+'\\n')\n",
    "        file1.close()\n",
    "      \n",
    "        print('Beginning file download with wget module')\n",
    "\n",
    "        url = 'https://vangoghmuseum.nl'+texts_cleaned[tamanio-1]\n",
    "        wget.download(url, texts_cleaned[0]+'\\ ' +texts_cleaned[0]+'.jpg')\n",
    "\n",
    "        # guardar-paginaweb.py\n",
    "\n",
    "        respuesta = urllib.request.urlopen(URL)\n",
    "        contenidoWeb = respuesta.read()\n",
    "\n",
    "        pag = open(texts_cleaned[0]+'.html', 'wb')\n",
    "        completeName2 = os.path.join(save_path,texts_cleaned[0]+'.html')    \n",
    "        pag.close()\n",
    "        file2 = open(completeName2, \"wb\")\n",
    "        file2.write(contenidoWeb)\n",
    "        file2.close()\n",
    "        \n",
    "        remove(texts_cleaned[0]+'.html')\n",
    "    else:\n",
    "        print (\"Status Code %d\" % status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "To offer you even more information about the museum and Vincent van Gogh, and serve you better, we use cookies.\n",
      "By clicking ‘Accept’, you are giving us permission to use these cookies. Cookies help us to ensure that the website works properly. We also analyse how the website is used, so that we can make any necessary improvements. Advertisements can also be displayed tailored to your interests. And finally, we use cookies to display forms, Google Maps and other embedded content.\n",
      "\n",
      "Find out more about our cookies.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "page = html.find('p').getText()\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning file download with wget module\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '.\\\\Congregation Leaving the Reformed Church in Nuenen\\\\daniela.jpgw7w3ytx9.tmp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-5b46983efcaf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'https://vangoghmuseum.nl/download/45214344-2680-447a-92d3-e34667866f59.jpg'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Congregation Leaving the Reformed Church in Nuenen\\daniela.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wget.py\u001b[0m in \u001b[0;36mdownload\u001b[1;34m(url, out, bar)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[1;31m# get filename for temp file in current directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[0mprefix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetect_filename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtmpfile\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtempfile\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkstemp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\".tmp\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdir\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\".\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munlink\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmpfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tempfile.py\u001b[0m in \u001b[0;36mmkstemp\u001b[1;34m(suffix, prefix, dir, text)\u001b[0m\n\u001b[0;32m    338\u001b[0m         \u001b[0mflags\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_bin_openflags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 340\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_mkstemp_inner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    341\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\tempfile.py\u001b[0m in \u001b[0;36m_mkstemp_inner\u001b[1;34m(dir, pre, suf, flags, output_type)\u001b[0m\n\u001b[0;32m    256\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpre\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msuf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[0mfd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0o600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mFileExistsError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             \u001b[1;32mcontinue\u001b[0m    \u001b[1;31m# try again\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '.\\\\Congregation Leaving the Reformed Church in Nuenen\\\\daniela.jpgw7w3ytx9.tmp'"
     ]
    }
   ],
   "source": [
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://vangoghmuseum.nl/download/45214344-2680-447a-92d3-e34667866f59.jpg'\n",
    "wget.download(url, 'Congregation Leaving the Reformed Church in Nuenen\\daniela.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plaster Cast of a Woman's Torso\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "URL = \"https://vangoghmuseum.nl/en/collection/s0199V1962\"\n",
    "\n",
    "def iterate_over_children(soup, texts):\n",
    "    for child in soup:\n",
    "        if child.string == None:\n",
    "            iterate_over_children(child, texts)\n",
    "        else:\n",
    "            if child.string != '\\n':\n",
    "                texts.append(child.string)\n",
    "\n",
    "# Realizamos la petición a la web\n",
    "req = requests.get(URL)\n",
    "\n",
    "# Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "status_code = req.status_code\n",
    "if status_code == 200:\n",
    "\n",
    "    # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "    html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "    article = html.find('section')\n",
    "    texts = []\n",
    "    iterate_over_children(article, texts)\n",
    "    texts_cleaned = [] \n",
    "    for text in texts:\n",
    "        cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "        texts_cleaned.append(cleaned_text)\n",
    "  #  print(texts_cleaned)\n",
    "       \n",
    "#Daniela Oject Data\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "    \n",
    "\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "    for i, entrada in enumerate(entradas):\n",
    "        # Con el método \"getText()\" no nos devuelve el HTML\n",
    "        hola= entrada.find('a').get('href') \n",
    "        texts_cleaned.append(hola)\n",
    "    print (texts_cleaned)\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "\n",
    "#url_imagen = \"https://golang.org/doc/gopher/appenginegophercolor.jpg\" # El link de la imagen\n",
    "#nombre_local_imagen = \"go.jpg\" # El nombre con el que queremos guardarla\n",
    "#imagen = requests.get(url_imagen).content\n",
    "#with open(nombre_local_imagen, 'wb') as handler:\n",
    "#\thandler.write(imagen)\n",
    "\n",
    "else:\n",
    "    print (\"Status Code %d\" % status_code)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Comedores de Patatas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path ='Comedores de Patatas'\n",
    "\n",
    "f = open('daniela6.txt', 'w')\n",
    "\n",
    "completeName = os.path.join(save_path,'daniela6.txt')         \n",
    "\n",
    "file1 = open(completeName, \"w\")\n",
    "\n",
    "file1.write(\"daniela carcamo\")\n",
    "file1.close()\n",
    "\n",
    "# remove(completeName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\r\\n<!DOCTYPE html>\\r\\n<html lang=\"en\" class=\"no-js\">\\r\\n\\r\\n<head>\\r\\n  <!-- 0d0b5046-1f3c-47ed-9928-b8403a6c0ed5 -->\\r\\n\\r\\n  <meta charset=\"utf-8\">\\r\\n  <meta name=\"google\" value=\"notranslate\">\\r\\n  <meta http-equiv=\"Content-Language\" content=\"en_US\" />\\r\\n  <meta http-equiv=\"X-UA-Compatible\" content=\"IE=edge,chrome'\n"
     ]
    }
   ],
   "source": [
    "# abre-paginaweb.py\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "print(contenidoWeb[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# guardar-paginaweb.py\n",
    "\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "f = open('patatas.html', 'wb')\n",
    "f.write(contenidoWeb)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<http.client.HTTPResponse object at 0x00000205800BA9E8>\n"
     ]
    }
   ],
   "source": [
    "print (respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
