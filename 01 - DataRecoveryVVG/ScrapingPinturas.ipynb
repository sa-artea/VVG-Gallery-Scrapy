{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraer datos de WEB\n",
    "\n",
    "Este Script se concentra en extraer la informacion y datos de la galeria del museo Van Gogh y las cartas escritas por el mismo artista para guardar los archivos de texto, png y otros datos necesarios para crear el set de datos de entrenamiento para un modelo de Machine Larning. Se utiliza BeautifulSoup para extraer la informacion desde los portales WEB de interes\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Extraer las imagenes, datos y metadatos de cada elemento dentro de la coleccion del museoVan Gogh.\n",
    "\n",
    "## Fuente de Datos\n",
    "\n",
    "Las fuentes de datos para el conjunto de datos son:\n",
    "\n",
    "* Galeria de obras y trabajo de Van Gogh: https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh\n",
    "* Archivo de cartas escritas por Van Gogh: http://vangoghletters.org/vg/search/simple?term=\n",
    "* Ruta de carpetas de recursos de las cartas escritas por Van Gogh: http://vangoghletters.org/vg/letters/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias necesarias para scrapyWEB\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# importar librerias para hacer un dataframe de referencia\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import os\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# from os import remove\n",
    "# import os.path\n",
    "# from os import remove\n",
    "# from numpy import array\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import requests, pandas, numpy, matplotlib.pyplot\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# import urllib\n",
    "# import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagina Fuente (Root Page)\n",
    "\n",
    "desde la pagina fuente recupero todos los enlaces a las obras de Vangohg y despues con la lista de enlaces recupero la informacion necesaria\n",
    "\n",
    "Enlaces de interes:\n",
    "- Extract links from webpage (BeautifulSoup): https://pythonspot.com/extract-links-from-webpage-beautifulsoup/\n",
    "- How to: Find all tags with some given name and attributes: https://kite.com/python/examples/1734/beautifulsoup-find-all-tags-with-some-given-name-and-attributes\n",
    "- Beautiful Soup can't find the part of the HTML I want: https://stackoverflow.com/questions/51982930/beautiful-soup-cant-find-the-part-of-the-html-i-want\n",
    "\n",
    "EXTRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galeria de Obras de vangoghmuseum.nl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion del DataFrame para la galeria\n",
    "defino las columnas necesarias del DataFrame para saber que he podido recuperar y que no, TRUE es que si lo tengo, FALSE de lo contrario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista de columnas para trabajar el DataFrame de la galeria, sirve para manejar la calidad del proceso\n",
    "# conocer los errores y ver que datos faltan por cada elemento de la galeria\n",
    "soupCol = [\n",
    "    \"ID\", # identificador unico de la galeria\n",
    "    \"ELEMENT_URL\", # enlace del elemento recuperado del ScrapyWEB\n",
    "    \"NAME\", # nombre del elemento en la galeria\n",
    "    \"DOWNLOAD_URL\", # enlace \n",
    "    \"HAS_DESCRIPTION\", # booleano que identifica si se tiene la seccion de descripcion en el HTML del elemento\n",
    "    \"HAS_DOWNLOAD\", # booleano que identifica si se tiene la seccion de enlace de descarga en el HTML del elemento\n",
    "    \"HAS_TAGS\", # booleano que identifica si se tiene la seccion de tags de busqueda en el HTML del elemento\n",
    "    \"HAS_DATA\", # booleano que identifica si se tiene la seccion de datos de archivo en el HTML del elemento\n",
    "    \"HAS_RELATEDW\", # booleano que identifica si se tiene la seccion de trabajo relacionado en el HTML del elemento\n",
    "    \"ERR_\", # si no se obtiene el nombre se guarda el error aca\n",
    "    \"ERR_DESCRIPTION\", # si no se obtiene el nombre se guarda el error aca \n",
    "    \"ERR_DOWNLOAD\", # si no se obtiene el enlace de descarga se guarda el error aca \n",
    "    \"ERR_TAGS\", # si no se obtiene los tags de busqueda se guarda el error aca \n",
    "    \"ERR_DATA\", # si no se obtiene los datos de archivo se guarda el error aca \n",
    "    \"ERR_RELATEDW\", # si no se obtiene el trabajo relacionado se guarda el error aca \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ELEMENT_URL</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DOWNLOAD_URL</th>\n",
       "      <th>HAS_DESCRIPTION</th>\n",
       "      <th>HAS_DOWNLOAD</th>\n",
       "      <th>HAS_TAGS</th>\n",
       "      <th>HAS_DATA</th>\n",
       "      <th>HAS_RELATEDW</th>\n",
       "      <th>ERR_</th>\n",
       "      <th>ERR_DESCRIPTION</th>\n",
       "      <th>ERR_DOWNLOAD</th>\n",
       "      <th>ERR_TAGS</th>\n",
       "      <th>ERR_DATA</th>\n",
       "      <th>ERR_RELATEDW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [ID, ELEMENT_URL, NAME, DOWNLOAD_URL, HAS_DESCRIPTION, HAS_DOWNLOAD, HAS_TAGS, HAS_DATA, HAS_RELATEDW, ERR_, ERR_DESCRIPTION, ERR_DOWNLOAD, ERR_TAGS, ERR_DATA, ERR_RELATEDW]\n",
       "Index: []"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creo el dataFrame vacio con las columnas necesarias\n",
    "soupFrame = pd.DataFrame(columns = soupCol)\n",
    "soupFrame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creando el indice de la coleccion\n",
    "creo el URL de busqueda en la pagina principal de la coleccion para extraer los URLs de los elementos creados por Van Gogh, para pruebas mantengo un numero limitado de URLs a recuperar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Search URL ---\n",
      "https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=1443\n"
     ]
    }
   ],
   "source": [
    "# Se recorre la pagina principal y recuperan los enlaces de l \n",
    "# numero de paginas/obras/enlaces a iterar\n",
    "\n",
    "paints = 9999\n",
    "defaultPaints = 50\n",
    "\n",
    "# maximo numero de pinturas conocidas\n",
    "maxPaints = 1443\n",
    "\n",
    "# busqueda base en la coleccion de Vangogh\n",
    "paintsSearch = \"https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=\"\n",
    "paintsRoot = \"https://vangoghmuseum.nl\"\n",
    "\n",
    "# por defecto pruebo 50 elementos de la coleccion\n",
    "paintsPage = paintsSearch + str(defaultPaints)\n",
    "\n",
    "# si el numero de enlaces a buscar esta en el rango a propiado\n",
    "if paints <= maxPaints and paints > 0:\n",
    "    paintsPage = paintsSearch + str(paints)\n",
    "\n",
    "# si el numero de enlaces es superior al maximo\n",
    "elif paints > maxPaints:\n",
    "    paintsPage = paintsSearch + str(maxPaints)\n",
    "\n",
    "# si hay algo raro\n",
    "else:\n",
    "    paintsPage = paintsSearch + str(defaultPaints)\n",
    "\n",
    "# reviso que cargue bien la pagina base\n",
    "print(\"--- Search URL ---\")\n",
    "print(paintsPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pido el HTML\n",
    "# chequeo si el enlace sirve para sacar la informacion\n",
    "\n",
    "# lista de enlaces, IDs nombres a elementos de la coleccion\n",
    "links = list()\n",
    "ids = list()\n",
    "names = list()\n",
    "\n",
    "# objetos necesarios para scrapy en beatifulsoup: handler y resultados\n",
    "soup = None\n",
    "linkSoup = list()\n",
    "nameSoup = list()\n",
    "# diccionario caracterisico para la busqueda para el enlace del elemento de la galeria\n",
    "linkAttr = {\"class\":\"link-teaser triggers-slideshow-effect\", \"href\":re.compile(\"^/en/collection/\")}\n",
    "# diccionario caracterisico para la busqueda del nombre del elemento de la galeria\n",
    "nameAttr = {\"class\":\"text-base text-dark\"}\n",
    "\n",
    "try:\n",
    "    # GET del URL\n",
    "    pageReq = requests.get(paintsPage)\n",
    "\n",
    "    # si el GET me responde bien con codigo 200\n",
    "    if pageReq.status_code == 200:\n",
    "\n",
    "        # uso beautifulSoup\n",
    "        soup = BeautifulSoup(pageReq.content, \"html.parser\")\n",
    "\n",
    "        # esta es la seccion del HTML donde estan los enlaces con tag 'a'\n",
    "        linkSoup = soup.body.findAll(\"a\", attrs = linkAttr)\n",
    "        \n",
    "        # esta es la seccion del HTML donde estan los enlaces con tag 'h3'\n",
    "        nameSoup = soup.body.findAll(\"h3\", attrs = nameAttr)\n",
    "\n",
    "# si algo pasa aca esto esta muy mal\n",
    "except Exception as e:\n",
    "    print(\"Error in request: \" + str(e))\n",
    "    print(\"Status Code: \" + str(pageReq.status_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saco los links e IDs a las paginas que quiero dentro de la coleccion\n",
    "# estan los elementos de tipo link-teaser y la expresion regular del href\n",
    "for link in linkSoup:\n",
    "    \n",
    "    # reconstruyo el enlace completo\n",
    "    # tempLink = paintsRoot + link.get('href')\n",
    "    tempLink = urllib.parse.urljoin(paintsRoot, link.get(\"href\"))\n",
    "    tempID = link.get(\"href\").replace(\"/en/collection/\", \"\")\n",
    "    ids.append(tempID)\n",
    "    links.append(tempLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saco los links e IDs a las paginas que quiero dentro de la coleccion\n",
    "# estan los elementos de tipo link-teaser y la expresion regular del href\n",
    "for name in nameSoup:\n",
    "    \n",
    "    # reconstruyo el enlace completo\n",
    "    # tempLink = paintsRoot + link.get('href')\n",
    "    tempName = re.sub(\"\\s+\", \" \", name.string)\n",
    "    tempName = tempName[1:-1]\n",
    "    names.append(tempName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agregos los detalles del scrapy en las columnas del dataframe de calidad (nombres, IDs y URLs)\n",
    "soupFrame[\"ID\"] = ids\n",
    "soupFrame[\"ELEMENT_URL\"] = links \n",
    "soupFrame[\"NAME\"] = names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links in search: 1443\n",
      "Repeated link elements: False\n",
      "Extracted IDs in search: 1443\n",
      "Repeated IDs elements: False\n",
      "Extracted names in search: 1443\n",
      "Repeated names elements: True\n",
      "Shape: (1443, 15)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1443 entries, 0 to 1442\n",
      "Data columns (total 15 columns):\n",
      "ID                 1443 non-null object\n",
      "ELEMENT_URL        1443 non-null object\n",
      "NAME               1443 non-null object\n",
      "DOWNLOAD_URL       0 non-null object\n",
      "HAS_DESCRIPTION    0 non-null object\n",
      "HAS_DOWNLOAD       0 non-null object\n",
      "HAS_TAGS           0 non-null object\n",
      "HAS_DATA           0 non-null object\n",
      "HAS_RELATEDW       0 non-null object\n",
      "ERR_               0 non-null object\n",
      "ERR_DESCRIPTION    0 non-null object\n",
      "ERR_DOWNLOAD       0 non-null object\n",
      "ERR_TAGS           0 non-null object\n",
      "ERR_DATA           0 non-null object\n",
      "ERR_RELATEDW       0 non-null object\n",
      "dtypes: object(15)\n",
      "memory usage: 169.2+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>ELEMENT_URL</th>\n",
       "      <th>NAME</th>\n",
       "      <th>DOWNLOAD_URL</th>\n",
       "      <th>HAS_DESCRIPTION</th>\n",
       "      <th>HAS_DOWNLOAD</th>\n",
       "      <th>HAS_TAGS</th>\n",
       "      <th>HAS_DATA</th>\n",
       "      <th>HAS_RELATEDW</th>\n",
       "      <th>ERR_</th>\n",
       "      <th>ERR_DESCRIPTION</th>\n",
       "      <th>ERR_DOWNLOAD</th>\n",
       "      <th>ERR_TAGS</th>\n",
       "      <th>ERR_DATA</th>\n",
       "      <th>ERR_RELATEDW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>1443</td>\n",
       "      <td>1443</td>\n",
       "      <td>1443</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>unique</td>\n",
       "      <td>1443</td>\n",
       "      <td>1443</td>\n",
       "      <td>956</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>top</td>\n",
       "      <td>d0414-007V1962</td>\n",
       "      <td>https://vangoghmuseum.nl/en/collection/d0381V1962</td>\n",
       "      <td>Head of a Woman, 1884 - 1885</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>freq</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID                                        ELEMENT_URL  \\\n",
       "count             1443                                               1443   \n",
       "unique            1443                                               1443   \n",
       "top     d0414-007V1962  https://vangoghmuseum.nl/en/collection/d0381V1962   \n",
       "freq                 1                                                  1   \n",
       "\n",
       "                                NAME DOWNLOAD_URL HAS_DESCRIPTION  \\\n",
       "count                           1443            0               0   \n",
       "unique                           956            0               0   \n",
       "top     Head of a Woman, 1884 - 1885          NaN             NaN   \n",
       "freq                              29          NaN             NaN   \n",
       "\n",
       "       HAS_DOWNLOAD HAS_TAGS HAS_DATA HAS_RELATEDW ERR_ ERR_DESCRIPTION  \\\n",
       "count             0        0        0            0    0               0   \n",
       "unique            0        0        0            0    0               0   \n",
       "top             NaN      NaN      NaN          NaN  NaN             NaN   \n",
       "freq            NaN      NaN      NaN          NaN  NaN             NaN   \n",
       "\n",
       "       ERR_DOWNLOAD ERR_TAGS ERR_DATA ERR_RELATEDW  \n",
       "count             0        0        0            0  \n",
       "unique            0        0        0            0  \n",
       "top             NaN      NaN      NaN          NaN  \n",
       "freq            NaN      NaN      NaN          NaN  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#chequeo la informacion recuperada de la busqueda\n",
    "print(\"Extracted links in search: \" + str(len(links)))\n",
    "print(\"Repeated link elements: \" + str(len(links) != len(set(links))))\n",
    "\n",
    "print(\"Extracted IDs in search: \" + str(len(ids)))\n",
    "print(\"Repeated IDs elements: \" + str(len(ids) != len(set(ids))))\n",
    "\n",
    "print(\"Extracted names in search: \" + str(len(names)))\n",
    "print(\"Repeated names elements: \" + str(len(names) != len(set(names))))\n",
    "\n",
    "print(\"Shape: \" + str(soupFrame.shape))\n",
    "\n",
    "soupFrame.info()\n",
    "soupFrame.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardando los links en TXT y CSV para backup del proceso y control de calidad\n",
    "galleryIndex = \"VanGoghGalleryIndex.txt\"\n",
    "galleryFile = \"VanGoghGalleryIndex.csv\"\n",
    "dataFolder = \"data\"\n",
    "\n",
    "# guardando archivo original de texto\n",
    "# si no existe el directorio\n",
    "if not os.path.exists(dataFolder):\n",
    "    \n",
    "    os.makedirs(dataFolder)\n",
    "\n",
    "# si existe el directorio\n",
    "else: \n",
    "    \n",
    "    # sobreescribo el archivo siempre\n",
    "    with open(os.path.join(os.getcwd(), dataFolder, galleryIndex), \"w\", encoding = \"utf-8\", errors = \"ignore\") as file:\n",
    "        for link in links:\n",
    "            file.write(\"%s\\n\" % link)\n",
    "    \n",
    "    soupFrame.to_csv(os.path.join(os.getcwd(), dataFolder, galleryFile), sep = \",\", encoding = \"utf-8\", mode = \"w\", na_rep = \"N.A.\")\n",
    "     \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directorio y Carpetas para persistencia\n",
    "configuro donde se va a guardar la informacion inicial dentro de OneDrive (gitHub no me deja), creo las carpetas y desacoplo el computador donde corre el script. Despues ejecuto los siguientes pasos:\n",
    "\n",
    "- Creo el folder fuente \"01 - Raw\"\n",
    "- Dentro del folder fuente creo los folderes \"01 - Paints\" y \"02 - Letters\"\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defino los directorios locales donde se va a persistir la informacion\n",
    "# no olvidarse del cambiar \"\\\" a \"\\\\\" en los filepath de windows por que si no no sirve nada\n",
    "# dir de santiago\n",
    "SFAM_ROOT = \"C:\\\\Users\\\\Felipe\\\\OneDrive - Universidad de Los Andes\\\\03 - PhD\\\\04 - Clases\\\\05 - IA en Arte y Disenho\\\\03 - Proyecto\\\\01 - Data\\\\01 - Raw\"\n",
    "# dir de daniela\n",
    "DCP_ROOT = \"\"\n",
    "# variable intermedia para independizar, se comenta uno u otro segun donde se corra\n",
    "WORK_ROOT = SFAM_ROOT\n",
    "# WORK_ROOT = DCP_ROOT\n",
    "\n",
    "# nombres de carpetas utiles donde se guarda la informacion\n",
    "rawFolder = \"01 - Raw\"\n",
    "paintsFolder = \"01 - Paints\"\n",
    "lettersFolder = \"02 - Letters\"\n",
    "\n",
    "#La ruta se obtiene con el dirpath y el filename y el dirpath.split(os.path.sep)[-1] agrega la clase\n",
    "DATA_ROOT = os.path.dirname(WORK_ROOT)\n",
    "print(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparar el directorio de trabajo\n",
    "# si la ruta de trabajo local esta definida\n",
    "workPath = \"\"\n",
    "\n",
    "# si no eexiste el ROOT apropiado, esto lo debe arreglar el usuario\n",
    "if not os.path.exists(WORK_ROOT):\n",
    "    \n",
    "    print(\"Make sure the WORK_ROOT is correct in your local HD!!!...\")\n",
    "\n",
    "# si si existe el root apropiado, configuro los folderes para guardar la informacion\n",
    "elif os.path.exists(WORK_ROOT):\n",
    "\n",
    "    # concateno los folders para la nueva carpeta de trabajo\n",
    "    workPath = os.path.join(DATA_ROOT, rawFolder, paintsFolder)\n",
    "#     workPath = os.path.join(DATA_ROOT, \"rawFolder\", \"paintsFolder\")\n",
    "\n",
    "    # si todo va bien y necesito crear nuevo folder de trabajo\n",
    "    if not os.path.exists(workPath):\n",
    "        \n",
    "        # creo folderes nuevos\n",
    "        os.makedirs(workPath)\n",
    "        print(\"creating proper folder configuration!!!...\")\n",
    "    \n",
    "    # si los folderes ya existen\n",
    "    if os.path.exists(workPath):\n",
    "        \n",
    "        print(\"Working folders and config are cool... carry on!...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# recorro el arreglo de enlaces habilitados para extraer la informacion\n",
    "# variables de conteo del proceso\n",
    "folderCount = 0\n",
    "downloadCount = 0\n",
    "linkCount = 0\n",
    "repeatCount = 0\n",
    "\n",
    "#inicio del ciclo para los enlaces\n",
    "for link in links:\n",
    "    \n",
    "    # recojo la informacion necesaria para crear las carpetas y guarar los datos\n",
    "    linkPar = urlparse(link)\n",
    "    linkLen = len(linkPar.path.split(\"/\"))\n",
    "    linkFolder = linkPar.path.split(\"/\")[linkLen-1]\n",
    "        \n",
    "    # chequeo si el enlace sirve para sacar la informacion\n",
    "    try:\n",
    "        # GET del URL\n",
    "        linkReq = requests.get(link)\n",
    "        \n",
    "        # si el GET me responde bien con codigo 200\n",
    "        if linkReq.status_code == 200:\n",
    "            \n",
    "            linkCount = linkCount + 1\n",
    "                    \n",
    "            # creo path para cada elemento en la coleccion\n",
    "            tempPaintFolder = os.path.join(workPath, linkFolder)\n",
    "\n",
    "            # creo la carpeta del nuevo elemento de la coleccion si no existe\n",
    "            if not os.path.exists(tempPaintFolder):\n",
    "\n",
    "                os.makedirs(tempPaintFolder)\n",
    "                # cuento cuantos folderes nuevos creo\n",
    "                folderCount = folderCount + 1\n",
    "\n",
    "            # si existe la carpeta, descargo la imagen de la obra dentro de la coleccion de una vez\n",
    "            if os.path.exists(tempPaintFolder):\n",
    "\n",
    "                # parse del cuerpo del elemento de la coleccion\n",
    "                linkSoup = BeautifulSoup(linkReq.content, \"html.parser\")\n",
    "\n",
    "                # busco todos los elementos de tipo class=\"button dark-hover\"\n",
    "                soupDict = {\"class\":\"button dark-hover\", \"href\":re.compile(\"^/download/\")}\n",
    "                downloadSoup = linkSoup.find('a', attrs = soupDict)\n",
    "\n",
    "                # creo el enlace para descargar la imagen\n",
    "                tempLink = urllib.parse.urljoin(paintsRoot, downloadSoup.get(\"href\"))\n",
    "\n",
    "                # pido el enlace de la imagen\n",
    "                downReq = requests.get(tempLink)\n",
    "\n",
    "                # creo el nombre y la direccion del archivo que quiero guardar en la carpeta\n",
    "                fileName = urlparse(tempLink)\n",
    "                fileName = fileName.path.split(\"/\")[len(fileName.path.split(\"/\"))-1]\n",
    "                filePath = os.path.join(tempPaintFolder, fileName)\n",
    "\n",
    "                # si el archivo no existe lo guardo\n",
    "                if not os.path.exists(filePath):\n",
    "                    \n",
    "#                     print(\"Downloding image from: \" + str(tempLink) + \" into: \" + str(filePath))\n",
    "                    print(\"Downloaded images: \" + str(downloadCount))\n",
    "\n",
    "                    with open(filePath, \"wb\") as file:\n",
    "                        file.write(downReq.content)\n",
    "                        file.close()\n",
    "                        downloadCount = downloadCount + 1\n",
    "\n",
    "                elif os.path.exists(filePath):\n",
    "                    repeatCount = repeatCount + 1\n",
    "#                     print(\"Repeated images: \" + str(repeatCount))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error in request: \" + str(e))\n",
    "        print(\"Status Code: \" + str(linkReq.status_code))\n",
    "        print(\"Status Code: \" + str(downReq.status_code))\n",
    "        print(\"Collection link: \" + str(link))\n",
    "        print(\"Image link: \" + str(tempLink))\n",
    "#         print(\"Explored links: \" + str(linkCount))\n",
    "        with open(filePath, \"wb\") as file:\n",
    "            file.write(downReq.content)\n",
    "            file.close()\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "print(\"Explored collection links: \" + str(linkCount))\n",
    "print(\"New folders created: \" + str(folderCount))\n",
    "print(\"Downloaded images: \" + str(downloadCount))\n",
    "print(\"Repeated images: \" + str(repeatCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de las pinturas en la galeria\n",
    "\n",
    "el proceso sigue los siguientes pasos, para facilidad de la estructura se propone un formato JSON por cada uno de los frames que se desean guardar en formato TXT, al final cada carpeta debe tener 4 archivos TXT mas una image en formato PNG. Los pasos a seguir son los siguientes:\n",
    "\n",
    "* Chequeo que los enlaces existan y creo las carpetas necesarias.\n",
    "* Extraigo las anotaciones de busqueda cada uno de los elementos de la galeria desde el objeto \"Search in the collection:\".\n",
    "* Extraigo los datos archivisticos de cada elemento de la galeria desde el objeto \"OBJECT DATA\".\n",
    "* Extraigo los trabajos relacionados si existen de cada elemento de la galeria desde el objeto \"Related work\".\n",
    "* Extraigo la imagen en formato PNG de cada elemento de la galeria en el desde el objeto \"DOWNLOAD IMAGE\"\n",
    "\n",
    "\n",
    " #### Enlaces utiles para el proceso\n",
    " \n",
    "- Como chequear que un enlace esta vivo: https://stackoverflow.com/questions/51639585/checking-if-a-website-exist-with-python3\n",
    "- Como utilizar libreria Request y el metodo GET: https://realpython.com/python-requests/\n",
    "- Manejo de errores en Python 3 URL 1: https://www.python-course.eu/python3_exception_handling.php\n",
    "- Manejo de errores en Python 3 URL 2: https://www.tutorialspoint.com/python3/python_exceptions.htm\n",
    "- descargar una imagen desde un URL: https://stackabuse.com/download-files-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivo de Cartas de vangoghletters.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "totalpinturas=1443\n",
    "temporal=199\n",
    "pagina='https://vangoghmuseum.nl/en/collection/s'\n",
    "a=np.arange(temporal)\n",
    "lista=[]\n",
    "for i in a:\n",
    "    lista.append(pagina+str((i+1)).zfill(4)+'V1962')\n",
    "    URL = lista[i]\n",
    "\n",
    "    def iterate_over_children(soup, texts):\n",
    "        for child in soup:\n",
    "            if child.string == None:\n",
    "                iterate_over_children(child, texts)\n",
    "            else:\n",
    "                if child.string != '\\n':\n",
    "                    texts.append(child.string)\n",
    "\n",
    "    # Realizamos la petición a la web\n",
    "    req = requests.get(URL)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    status_code = req.status_code\n",
    "    if status_code == 200:\n",
    "\n",
    "        # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "        html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "        article = html.find('section')\n",
    "        texts = []\n",
    "        iterate_over_children(article, texts)\n",
    "        texts_cleaned = [] \n",
    "        for text in texts:\n",
    "            cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "            texts_cleaned.append(cleaned_text)\n",
    "\n",
    "    #Daniela Oject Data\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "\n",
    "\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        for i, entrada in enumerate(entradas):\n",
    "            # Con el método \"getText()\" no nos devuelve el HTML\n",
    "            hola= entrada.find('a').get('href') \n",
    "            texts_cleaned.append(hola)\n",
    "            \n",
    "        #links del parrafo (puede ir a las cartas)\n",
    "        link_p=[tag['href'] for tag in html.select('p a[href]')]\n",
    "        #links de search in the collection\n",
    "        link_c=[tag['href'] for tag in html.select('aside a[href]')]\n",
    "            \n",
    "        \n",
    "        #print(texts_cleaned)\n",
    "        os.mkdir(texts_cleaned[0])\n",
    "        #print (texts_cleaned)\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        save_path =texts_cleaned[0]\n",
    "\n",
    "        f=open(texts_cleaned[0]+'.txt', 'w')\n",
    "\n",
    "        completeName = os.path.join(save_path,texts_cleaned[0]+'.txt')    \n",
    "        \n",
    "        f.close()\n",
    "        remove(texts_cleaned[0]+'.txt')\n",
    "\n",
    "        file1 = open(completeName, \"w\")\n",
    "        \n",
    "        tamanio = len(texts_cleaned)\n",
    "        for i in range(0,len(texts_cleaned)):\n",
    "            file1.write(texts_cleaned[i]+'\\n')\n",
    "            \n",
    "        for i in range(0,len(link_p)):\n",
    "            file1.write(link_p[i]+'\\n')\n",
    "        for i in range(0,len(link_c)):\n",
    "            file1.write(link_c[i]+'\\n')\n",
    "        file1.close()\n",
    "      \n",
    "        print('Beginning file download with wget module')\n",
    "\n",
    "        url = 'https://vangoghmuseum.nl'+texts_cleaned[tamanio-1]\n",
    "        wget.download(url, texts_cleaned[0]+'\\ ' +texts_cleaned[0]+'.jpg')\n",
    "\n",
    "        # guardar-paginaweb.py\n",
    "\n",
    "        respuesta = urllib.request.urlopen(URL)\n",
    "        contenidoWeb = respuesta.read()\n",
    "\n",
    "        pag = open(texts_cleaned[0]+'.html', 'wb')\n",
    "        completeName2 = os.path.join(save_path,texts_cleaned[0]+'.html')    \n",
    "        pag.close()\n",
    "        file2 = open(completeName2, \"wb\")\n",
    "        file2.write(contenidoWeb)\n",
    "        file2.close()\n",
    "        \n",
    "        remove(texts_cleaned[0]+'.html')\n",
    "    else:\n",
    "        print (\"Status Code %d\" % status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = html.find('p').getText()\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://vangoghmuseum.nl/download/45214344-2680-447a-92d3-e34667866f59.jpg'\n",
    "wget.download(url, 'Congregation Leaving the Reformed Church in Nuenen\\daniela.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "URL = \"https://vangoghmuseum.nl/en/collection/s0199V1962\"\n",
    "\n",
    "def iterate_over_children(soup, texts):\n",
    "    for child in soup:\n",
    "        if child.string == None:\n",
    "            iterate_over_children(child, texts)\n",
    "        else:\n",
    "            if child.string != '\\n':\n",
    "                texts.append(child.string)\n",
    "\n",
    "# Realizamos la petición a la web\n",
    "req = requests.get(URL)\n",
    "\n",
    "# Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "status_code = req.status_code\n",
    "if status_code == 200:\n",
    "\n",
    "    # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "    html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "    article = html.find('section')\n",
    "    texts = []\n",
    "    iterate_over_children(article, texts)\n",
    "    texts_cleaned = [] \n",
    "    for text in texts:\n",
    "        cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "        texts_cleaned.append(cleaned_text)\n",
    "  #  print(texts_cleaned)\n",
    "       \n",
    "#Daniela Oject Data\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "    \n",
    "\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "    for i, entrada in enumerate(entradas):\n",
    "        # Con el método \"getText()\" no nos devuelve el HTML\n",
    "        hola= entrada.find('a').get('href') \n",
    "        texts_cleaned.append(hola)\n",
    "    print (texts_cleaned)\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "\n",
    "#url_imagen = \"https://golang.org/doc/gopher/appenginegophercolor.jpg\" # El link de la imagen\n",
    "#nombre_local_imagen = \"go.jpg\" # El nombre con el que queremos guardarla\n",
    "#imagen = requests.get(url_imagen).content\n",
    "#with open(nombre_local_imagen, 'wb') as handler:\n",
    "#\thandler.write(imagen)\n",
    "\n",
    "else:\n",
    "    print (\"Status Code %d\" % status_code)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Comedores de Patatas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path ='Comedores de Patatas'\n",
    "\n",
    "f = open('daniela6.txt', 'w')\n",
    "\n",
    "completeName = os.path.join(save_path,'daniela6.txt')         \n",
    "\n",
    "file1 = open(completeName, \"w\")\n",
    "\n",
    "file1.write(\"daniela carcamo\")\n",
    "file1.close()\n",
    "\n",
    "# remove(completeName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abre-paginaweb.py\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "print(contenidoWeb[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar-paginaweb.py\n",
    "\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "f = open('patatas.html', 'wb')\n",
    "f.write(contenidoWeb)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
