{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraer datos de WEB\n",
    "\n",
    "Este Script se concentra en extraer la informacion y datos de la galeria del museo Van Gogh y las cartas escritas por el mismo artista para guardar los archivos de texto, png y otros datos necesarios para crear el set de datos de entrenamiento para un modelo de Machine Larning. Se utiliza BeautifulSoup para extraer la informacion desde los portales WEB de interes\n",
    "\n",
    "## Objetivos\n",
    "\n",
    "Extraer las imagenes, datos y metadatos de cada elemento dentro de la coleccion del museoVan Gogh.\n",
    "\n",
    "## Fuente de Datos\n",
    "\n",
    "Las fuentes de datos para el conjunto de datos son:\n",
    "\n",
    "* Galeria de obras y trabajo de Van Gogh: https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh\n",
    "* Archivo de cartas escritas por Van Gogh: http://vangoghletters.org/vg/search/simple?term=\n",
    "* Ruta de carpetas de recursos de las cartas escritas por Van Gogh: http://vangoghletters.org/vg/letters/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importar librerias necesarias\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# import os\n",
    "# import urllib.request, urllib.error, urllib.parse\n",
    "# from os import remove\n",
    "# import os.path\n",
    "# from os import remove\n",
    "# from numpy import array\n",
    "# from bs4 import BeautifulSoup\n",
    "# import requests\n",
    "# import requests, pandas, numpy, matplotlib.pyplot\n",
    "# from bs4 import BeautifulSoup\n",
    "# import re\n",
    "# import urllib\n",
    "# import wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pagina Fuente (Root Page)\n",
    "\n",
    "desde la pagina fuente recupero todos los enlaces a las obras de Vangohg y despues con la lista de enlaces recupero la informacion necesaria\n",
    "\n",
    "Enlaces de interes:\n",
    "- Extract links from webpage (BeautifulSoup): https://pythonspot.com/extract-links-from-webpage-beautifulsoup/\n",
    "- How to: Find all tags with some given name and attributes: https://kite.com/python/examples/1734/beautifulsoup-find-all-tags-with-some-given-name-and-attributes\n",
    "- Beautiful Soup can't find the part of the HTML I want: https://stackoverflow.com/questions/51982930/beautiful-soup-cant-find-the-part-of-the-html-i-want\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galeria de Obras de vangoghmuseum.nl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Search URL ---\n",
      "https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=1443\n"
     ]
    }
   ],
   "source": [
    "# Se recorre la pagina principal y recuperan los enlaces de l \n",
    "# numero de paginas/obras/enlaces a iterar\n",
    "\n",
    "paints = 1443\n",
    "defaultPaints = 50\n",
    "\n",
    "# maximo numero de pinturas conocidas\n",
    "maxPaints = 1443\n",
    "\n",
    "# busqueda base en la coleccion de Vangogh\n",
    "paintsSearch = \"https://vangoghmuseum.nl/en/search/collection?q=&artist=Vincent%20van%20Gogh&pagesize=\"\n",
    "paintsRoot = \"https://vangoghmuseum.nl\"\n",
    "\n",
    "# por defecto pruebo 50 elementos de la coleccion\n",
    "paintsPage = paintsSearch + str(defaultPaints)\n",
    "\n",
    "# si el numero de enlaces abuscar es menor al maximo de la coleccion conocida\n",
    "if paints <= maxPaints:\n",
    "    paintsPage = paintsSearch + str(paints)\n",
    "\n",
    "# si hay algo raro\n",
    "else:\n",
    "    paintsPage = paintsSearch + str(defaultPaints)\n",
    "\n",
    "# reviso que cargue bien la pagina base\n",
    "print(\"--- Search URL ---\")\n",
    "print(paintsPage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pido el HTML\n",
    "# chequeo si el enlace sirve para sacar la informacion\n",
    "\n",
    "# lista de enlaces a elementos de la coleccion\n",
    "links = list()\n",
    "soup = None\n",
    "linkSoup = list()\n",
    "\n",
    "try:\n",
    "    # GET del URL\n",
    "    pageReq = requests.get(paintsPage)\n",
    "\n",
    "    # si el GET me responde bien con codigo 200\n",
    "    if pageReq.status_code == 200:\n",
    "\n",
    "        # uso beautifulSoup\n",
    "        soup = BeautifulSoup(pageReq.content, \"html.parser\")\n",
    "\n",
    "        # esta es la seccion del HTML donde estan los resultados de la coleccion NO LA NECESITO POR AHORA\n",
    "        # collectionSoup = soup.body.findAll(\"ul\", attrs = {\"class\":\"cols cols-3up list-plain\"})\n",
    "        linkSoup = soup.body.findAll('a', attrs = {\"class\":\"link-teaser triggers-slideshow-effect\", \"href\":re.compile(\"^/en/collection/\")})\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"Error in request: \" + str(e))\n",
    "    print(\"Status Code: \" + str(pageReq.status_code))\n",
    "\n",
    "# saco los links a las paginas que quiero dentro de la busqueda de coleccion\n",
    "# estan los elementos de tipo link-teaser y la expresion regular del href\n",
    "for link in linkSoup:\n",
    "    \n",
    "    # reconstruyo el enlace completo\n",
    "    # tempLink = paintsRoot + link.get('href')\n",
    "    tempLink = urllib.parse.urljoin(paintsRoot, link.get('href'))\n",
    "    links.append(tempLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted links in search: 1443\n",
      "Repeated link elements: False\n"
     ]
    }
   ],
   "source": [
    "#chequeo los links de la busqueda\n",
    "print(\"Extracted links in search: \" + str(len(links)))\n",
    "print(\"Repeated link elements: \" + str(len(links) != len(set(links))))\n",
    "for link in links:\n",
    "    pass\n",
    "#     print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardando los links en TXT para backup\n",
    "urlIndexName = \"VanGoghGalleryIndex.txt\"\n",
    "dataFolder = \"01-Data\"\n",
    "\n",
    "if not os.path.exists(dataFolder):\n",
    "    \n",
    "    os.makedirs(dataFolder)\n",
    "    \n",
    "else: \n",
    "\n",
    "    with open(os.path.join(os.getcwd(), dataFolder, urlIndexName), 'w') as file:\n",
    "        for item in links:\n",
    "            file.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Directorio y Carpetas para persistencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Felipe\\OneDrive - Universidad de Los Andes\\03 - PhD\\04 - Clases\\05 - IA en Arte y Disenho\\03 - Proyecto\\01 - Data\n"
     ]
    }
   ],
   "source": [
    "# defino los directorios locales donde se va a persistir la informacion\n",
    "# no olvidarse del cambiar \"\\\" a \"\\\\\" en los filepath de windows por que si no no sirve nada\n",
    "# dir de santiago\n",
    "SFAM_ROOT = \"C:\\\\Users\\\\Felipe\\\\OneDrive - Universidad de Los Andes\\\\03 - PhD\\\\04 - Clases\\\\05 - IA en Arte y Disenho\\\\03 - Proyecto\\\\01 - Data\\\\01 - Raw\"\n",
    "# dir de daniela\n",
    "DCP_ROOT = \"\"\n",
    "# variable intermedia para independizar, se comenta uno u otro segun donde se corra\n",
    "WORK_ROOT = SFAM_ROOT\n",
    "# WORK_ROOT = DCP_ROOT\n",
    "\n",
    "# nombres de carpetas utiles donde se guarda la informacion\n",
    "rawFolder = \"01 - Raw\"\n",
    "paintsFolder = \"01 - Paints\"\n",
    "lettersFolder = \"02 - Letters\"\n",
    "\n",
    "#La ruta se obtiene con el dirpath y el filename y el dirpath.split(os.path.sep)[-1] agrega la clase\n",
    "DATA_ROOT = os.path.dirname(WORK_ROOT)\n",
    "print(DATA_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicion de funciones para extraer Informacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Procesamiento de las pinturas en la galeria\n",
    "\n",
    "el proceso sigue los siguientes pasos, para facilidad de la estructura se propone un formato JSON por cada uno de los frames que se desean guardar en formato TXT, al final cada carpeta debe tener 4 archivos TXT mas una image en formato PNG. Los pasos a seguir son los siguientes:\n",
    "\n",
    "* Chequeo que los enlaces existan y creo las carpetas necesarias.\n",
    "* Extraigo las anotaciones de busqueda cada uno de los elementos de la galeria desde el objeto \"Search in the collection:\".\n",
    "* Extraigo los datos archivisticos de cada elemento de la galeria desde el objeto \"OBJECT DATA\".\n",
    "* Extraigo los trabajos relacionados si existen de cada elemento de la galeria desde el objeto \"Related work\".\n",
    "* Extraigo la imagen en formato PNG de cada elemento de la galeria en el desde el objeto \"DOWNLOAD IMAGE\"\n",
    "\n",
    "\n",
    " #### Enlaces utiles para el proceso\n",
    " \n",
    "- Como chequear que un enlace esta vivo: https://stackoverflow.com/questions/51639585/checking-if-a-website-exist-with-python3\n",
    "- Como utilizar libreria Request y el metodo GET: https://realpython.com/python-requests/\n",
    "- Manejo de errores en Python 3 URL 1: https://www.python-course.eu/python3_exception_handling.php\n",
    "- Manejo de errores en Python 3 URL 2: https://www.tutorialspoint.com/python3/python_exceptions.htm\n",
    "- descargar una imagen desde un URL: https://stackabuse.com/download-files-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working folders and config are cool... carry on!...\n"
     ]
    }
   ],
   "source": [
    "# preparar el directorio de trabajo\n",
    "# si la ruta de trabajo local esta definida\n",
    "workPath = \"\"\n",
    "\n",
    "# si no eexiste el ROOT apropiado, esto lo debe arreglar el usuario\n",
    "if not os.path.exists(WORK_ROOT):\n",
    "    \n",
    "    print(\"Make sure the WORK_ROOT is correct in your local HD!!!...\")\n",
    "\n",
    "# si si existe el root apropiado, configuro los folderes para guardar la informacion\n",
    "elif os.path.exists(WORK_ROOT):\n",
    "\n",
    "    # concateno los folders para la nueva carpeta de trabajo\n",
    "    workPath = os.path.join(DATA_ROOT, rawFolder, paintsFolder)\n",
    "#     workPath = os.path.join(DATA_ROOT, \"rawFolder\", \"paintsFolder\")\n",
    "\n",
    "    # si todo va bien y necesito crear nuevo folder de trabajo\n",
    "    if not os.path.exists(workPath):\n",
    "        \n",
    "        # creo folderes nuevos\n",
    "        os.makedirs(workPath)\n",
    "        print(\"creating proper folder configuration!!!...\")\n",
    "    \n",
    "    # si los folderes ya existen\n",
    "    if os.path.exists(workPath):\n",
    "        \n",
    "        print(\"Working folders and config are cool... carry on!...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error in request: 'NoneType' object has no attribute 'get'\n",
      "Status Code: 200\n",
      "Status Code: 200\n",
      "Collection link: https://vangoghmuseum.nl/en/collection/d0031V1962v\n",
      "Image link: https://vangoghmuseum.nl/download/111cbd85-420d-48f9-aa1b-9e221636e2c7.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-72b780c30068>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m                 \u001b[1;31m# pido el enlace de la imagen\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m                 \u001b[0mdownReq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtempLink\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m                 \u001b[1;31m# creo el nombre y la direccion del archivo que quiero guardar en la carpeta\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    531\u001b[0m         }\n\u001b[0;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 533\u001b[1;33m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    534\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    535\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36msend\u001b[1;34m(self, request, **kwargs)\u001b[0m\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    685\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m             \u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mcontent\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    826\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 828\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miter_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONTENT_CHUNK_SIZE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34mb''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    829\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_content_consumed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m()\u001b[0m\n\u001b[0;32m    748\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'stream'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 750\u001b[1;33m                     \u001b[1;32mfor\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    751\u001b[0m                         \u001b[1;32myield\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    752\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mProtocolError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mstream\u001b[1;34m(self, amt, decode_content)\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m             \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_fp_closed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 465\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecode_content\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecode_content\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    466\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\site-packages\\urllib3\\response.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt, decode_content, cache_content)\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m                 \u001b[0mcache_content\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Platform-specific: Buggy versions of Python.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m                     \u001b[1;31m# Close the connection when no data is returned\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    445\u001b[0m             \u001b[1;31m# Amount is given, implement using readinto\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    446\u001b[0m             \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 447\u001b[1;33m             \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    448\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmemoryview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    449\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\http\\client.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[1;31m# connection, and the user is reading more bytes than will be provided\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    490\u001b[0m         \u001b[1;31m# (for example, reading in 1k chunks)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 491\u001b[1;33m         \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadinto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    492\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    493\u001b[0m             \u001b[1;31m# Ideally, we would raise IncompleteRead if the content-length\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[1;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[0;32m   1047\u001b[0m                   \u001b[1;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1048\u001b[0m                   self.__class__)\n\u001b[1;32m-> 1049\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1050\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1051\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python37\\lib\\ssl.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, len, buffer)\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 908\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    909\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# recorro el arreglo de enlaces habilitados para extraer la informacion\n",
    "# variables de conteo del proceso\n",
    "folderCount = 0\n",
    "downloadCount = 0\n",
    "linkCount = 0\n",
    "repeatCount = 0\n",
    "\n",
    "#inicio del ciclo para los enlaces\n",
    "for link in links:\n",
    "    \n",
    "    # recojo la informacion necesaria para crear las carpetas y guarar los datos\n",
    "    linkPar = urlparse(link)\n",
    "    linkLen = len(linkPar.path.split(\"/\"))\n",
    "    linkFolder = linkPar.path.split(\"/\")[linkLen-1]\n",
    "        \n",
    "    # chequeo si el enlace sirve para sacar la informacion\n",
    "    try:\n",
    "        # GET del URL\n",
    "        linkReq = requests.get(link)\n",
    "        \n",
    "        # si el GET me responde bien con codigo 200\n",
    "        if linkReq.status_code == 200:\n",
    "            \n",
    "            linkCount = linkCount + 1\n",
    "                    \n",
    "            # creo path para cada elemento en la coleccion\n",
    "            tempPaintFolder = os.path.join(workPath, linkFolder)\n",
    "\n",
    "            # creo la carpeta del nuevo elemento de la coleccion si no existe\n",
    "            if not os.path.exists(tempPaintFolder):\n",
    "\n",
    "                os.makedirs(tempPaintFolder)\n",
    "                # cuento cuantos folderes nuevos creo\n",
    "                folderCount = folderCount + 1\n",
    "\n",
    "            # si existe la carpeta, descargo la imagen de la obra dentro de la coleccion de una vez\n",
    "            if os.path.exists(tempPaintFolder):\n",
    "\n",
    "                # parse del cuerpo del elemento de la coleccion\n",
    "                linkSoup = BeautifulSoup(linkReq.content, \"html.parser\")\n",
    "\n",
    "                # busco todos los elementos de tipo class=\"button dark-hover\"\n",
    "                soupDict = {\"class\":\"button dark-hover\", \"href\":re.compile(\"^/download/\")}\n",
    "                downloadSoup = linkSoup.find('a', attrs = soupDict)\n",
    "\n",
    "                # creo el enlace para descargar la imagen\n",
    "                tempLink = urllib.parse.urljoin(paintsRoot, downloadSoup.get(\"href\"))\n",
    "\n",
    "                # pido el enlace de la imagen\n",
    "                downReq = requests.get(tempLink)\n",
    "\n",
    "                # creo el nombre y la direccion del archivo que quiero guardar en la carpeta\n",
    "                fileName = urlparse(tempLink)\n",
    "                fileName = fileName.path.split(\"/\")[len(fileName.path.split(\"/\"))-1]\n",
    "                filePath = os.path.join(tempPaintFolder, fileName)\n",
    "\n",
    "                # si el archivo no existe lo guardo\n",
    "                if not os.path.exists(filePath):\n",
    "                    \n",
    "#                     print(\"Downloding image from: \" + str(tempLink) + \" into: \" + str(filePath))\n",
    "                    print(\"Downloaded images: \" + str(downloadCount))\n",
    "\n",
    "                    with open(filePath, \"wb\") as file:\n",
    "                        file.write(downReq.content)\n",
    "                        file.close()\n",
    "                        downloadCount = downloadCount + 1\n",
    "\n",
    "                elif os.path.exists(filePath):\n",
    "                    repeatCount = repeatCount + 1\n",
    "#                     print(\"Repeated images: \" + str(repeatCount))\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(\"Error in request: \" + str(e))\n",
    "        print(\"Status Code: \" + str(linkReq.status_code))\n",
    "        print(\"Status Code: \" + str(downReq.status_code))\n",
    "        print(\"Collection link: \" + str(link))\n",
    "        print(\"Image link: \" + str(tempLink))\n",
    "#         print(\"Explored links: \" + str(linkCount))\n",
    "        with open(filePath, \"wb\") as file:\n",
    "            file.write(downReq.content)\n",
    "            file.close()\n",
    "            downloadCount = downloadCount + 1\n",
    "\n",
    "print(\"Explored collection links: \" + str(linkCount))\n",
    "print(\"New folders created: \" + str(folderCount))\n",
    "print(\"Downloaded images: \" + str(downloadCount))\n",
    "print(\"Repeated images: \" + str(repeatCount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Archivo de Cartas de vangoghletters.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "totalpinturas=1443\n",
    "temporal=199\n",
    "pagina='https://vangoghmuseum.nl/en/collection/s'\n",
    "a=np.arange(temporal)\n",
    "lista=[]\n",
    "for i in a:\n",
    "    lista.append(pagina+str((i+1)).zfill(4)+'V1962')\n",
    "    URL = lista[i]\n",
    "\n",
    "    def iterate_over_children(soup, texts):\n",
    "        for child in soup:\n",
    "            if child.string == None:\n",
    "                iterate_over_children(child, texts)\n",
    "            else:\n",
    "                if child.string != '\\n':\n",
    "                    texts.append(child.string)\n",
    "\n",
    "    # Realizamos la petición a la web\n",
    "    req = requests.get(URL)\n",
    "\n",
    "    # Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "    status_code = req.status_code\n",
    "    if status_code == 200:\n",
    "\n",
    "        # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "        html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "        article = html.find('section')\n",
    "        texts = []\n",
    "        iterate_over_children(article, texts)\n",
    "        texts_cleaned = [] \n",
    "        for text in texts:\n",
    "            cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "            texts_cleaned.append(cleaned_text)\n",
    "\n",
    "    #Daniela Oject Data\n",
    "        # Obtenemos todos los divs donde están las entradas\n",
    "        entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "\n",
    "\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        for i, entrada in enumerate(entradas):\n",
    "            # Con el método \"getText()\" no nos devuelve el HTML\n",
    "            hola= entrada.find('a').get('href') \n",
    "            texts_cleaned.append(hola)\n",
    "            \n",
    "        #links del parrafo (puede ir a las cartas)\n",
    "        link_p=[tag['href'] for tag in html.select('p a[href]')]\n",
    "        #links de search in the collection\n",
    "        link_c=[tag['href'] for tag in html.select('aside a[href]')]\n",
    "            \n",
    "        \n",
    "        #print(texts_cleaned)\n",
    "        os.mkdir(texts_cleaned[0])\n",
    "        #print (texts_cleaned)\n",
    "        # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "        save_path =texts_cleaned[0]\n",
    "\n",
    "        f=open(texts_cleaned[0]+'.txt', 'w')\n",
    "\n",
    "        completeName = os.path.join(save_path,texts_cleaned[0]+'.txt')    \n",
    "        \n",
    "        f.close()\n",
    "        remove(texts_cleaned[0]+'.txt')\n",
    "\n",
    "        file1 = open(completeName, \"w\")\n",
    "        \n",
    "        tamanio = len(texts_cleaned)\n",
    "        for i in range(0,len(texts_cleaned)):\n",
    "            file1.write(texts_cleaned[i]+'\\n')\n",
    "            \n",
    "        for i in range(0,len(link_p)):\n",
    "            file1.write(link_p[i]+'\\n')\n",
    "        for i in range(0,len(link_c)):\n",
    "            file1.write(link_c[i]+'\\n')\n",
    "        file1.close()\n",
    "      \n",
    "        print('Beginning file download with wget module')\n",
    "\n",
    "        url = 'https://vangoghmuseum.nl'+texts_cleaned[tamanio-1]\n",
    "        wget.download(url, texts_cleaned[0]+'\\ ' +texts_cleaned[0]+'.jpg')\n",
    "\n",
    "        # guardar-paginaweb.py\n",
    "\n",
    "        respuesta = urllib.request.urlopen(URL)\n",
    "        contenidoWeb = respuesta.read()\n",
    "\n",
    "        pag = open(texts_cleaned[0]+'.html', 'wb')\n",
    "        completeName2 = os.path.join(save_path,texts_cleaned[0]+'.html')    \n",
    "        pag.close()\n",
    "        file2 = open(completeName2, \"wb\")\n",
    "        file2.write(contenidoWeb)\n",
    "        file2.close()\n",
    "        \n",
    "        remove(texts_cleaned[0]+'.html')\n",
    "    else:\n",
    "        print (\"Status Code %d\" % status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = html.find('p').getText()\n",
    "print(page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "print('Beginning file download with wget module')\n",
    "\n",
    "url = 'https://vangoghmuseum.nl/download/45214344-2680-447a-92d3-e34667866f59.jpg'\n",
    "wget.download(url, 'Congregation Leaving the Reformed Church in Nuenen\\daniela.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "URL = \"https://vangoghmuseum.nl/en/collection/s0199V1962\"\n",
    "\n",
    "def iterate_over_children(soup, texts):\n",
    "    for child in soup:\n",
    "        if child.string == None:\n",
    "            iterate_over_children(child, texts)\n",
    "        else:\n",
    "            if child.string != '\\n':\n",
    "                texts.append(child.string)\n",
    "\n",
    "# Realizamos la petición a la web\n",
    "req = requests.get(URL)\n",
    "\n",
    "# Comprobamos que la petición nos devuelve un Status Code = 200\n",
    "status_code = req.status_code\n",
    "if status_code == 200:\n",
    "\n",
    "    # Pasamos el contenido HTML de la web a un objeto BeautifulSoup()\n",
    "    html = BeautifulSoup(req.text, \"html.parser\")\n",
    "\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    #entradas = html.find_all('div', {'class': 'page-unit'})\n",
    "    article = html.find('section')\n",
    "    texts = []\n",
    "    iterate_over_children(article, texts)\n",
    "    texts_cleaned = [] \n",
    "    for text in texts:\n",
    "        cleaned_text = text.replace(\"\\n\", \"\").strip()\n",
    "        texts_cleaned.append(cleaned_text)\n",
    "  #  print(texts_cleaned)\n",
    "       \n",
    "#Daniela Oject Data\n",
    "    # Obtenemos todos los divs donde están las entradas\n",
    "    entradas = html.find_all('div', {'class': 'download-buttons-container'})\n",
    "    \n",
    "\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "    for i, entrada in enumerate(entradas):\n",
    "        # Con el método \"getText()\" no nos devuelve el HTML\n",
    "        hola= entrada.find('a').get('href') \n",
    "        texts_cleaned.append(hola)\n",
    "    print (texts_cleaned)\n",
    "    # Recorremos todas las entradas para extraer el título, autor y fecha\n",
    "\n",
    "#url_imagen = \"https://golang.org/doc/gopher/appenginegophercolor.jpg\" # El link de la imagen\n",
    "#nombre_local_imagen = \"go.jpg\" # El nombre con el que queremos guardarla\n",
    "#imagen = requests.get(url_imagen).content\n",
    "#with open(nombre_local_imagen, 'wb') as handler:\n",
    "#\thandler.write(imagen)\n",
    "\n",
    "else:\n",
    "    print (\"Status Code %d\" % status_code)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('Comedores de Patatas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "save_path ='Comedores de Patatas'\n",
    "\n",
    "f = open('daniela6.txt', 'w')\n",
    "\n",
    "completeName = os.path.join(save_path,'daniela6.txt')         \n",
    "\n",
    "file1 = open(completeName, \"w\")\n",
    "\n",
    "file1.write(\"daniela carcamo\")\n",
    "file1.close()\n",
    "\n",
    "# remove(completeName)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abre-paginaweb.py\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "print(contenidoWeb[0:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guardar-paginaweb.py\n",
    "\n",
    "url = 'https://www.vangoghmuseum.nl/en/collection/s0005V1962?v=1'\n",
    "\n",
    "respuesta = urllib.request.urlopen(url)\n",
    "contenidoWeb = respuesta.read()\n",
    "\n",
    "f = open('patatas.html', 'wb')\n",
    "f.write(contenidoWeb)\n",
    "f.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (respuesta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
